{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lightstream","text":"<p>Lightstream is a Pytorch-Lightning library for training CNN-based models with large input data using streaming.  This approach allows you to parse huge (image) inputs through a CNN without running into memory bottlenecks, i.e. getting GPU out of memory (OOM) errors.</p> <p>The underlying algorithm is based on the <code>streaming</code> paper described in [1]. During training/inferencing,  a huge input image that would normally cause GPU OOM is split into tiles and processed sequentially until a pre-defined part of the network.  There, the individual tiles are stitched back together, and the forward/backward is finished normally. Due to gradient  checkpointing, intermediate features are deleted to save memory, and are re-computed tile-wise during backpropagation (see figure below).</p> <p>By doing so, the result is mathematically the same as if the large input was parsed directly through a GPU without memory restrictions.</p> <p>Figure 1: Alt Text</p>"},{"location":"#implemented-in-pytorch-lightning","title":"Implemented in Pytorch-Lightning","text":"<p>The Lightstream package is simple to test and extend as it works with native Pytorch, and also works with Lightning to minimize boilerplate code. Most convolutional neural networks can be easily converted into a streaming equivalent using a simple wrapper in native Pytorch:</p> <pre><code># Resnet18 turned into a streaming-capable network\nfrom torchvision.models import resnet18\nimport torch.nn as nn\n\nstream_layers = nn.Sequential(\n        net.conv1, net.bn1, net.relu, net.maxpool, net.layer1, net.layer2, net.layer3, net.layer4\n    )\n\n\nsCNN = StreamingCNN(stream_layers, tile_shape=(1, 3, 600, 600))\nstr_output = sCNN(image)\n\nfinal_output = final_layers(str_output)\nloss = criterion(final_output, labels)\n\nloss.backward()\nsCNN.backward(image, str_output.grad)\n</code></pre> <p>Warning</p> <p>Not all layers are supported during streaming. To see the caveats, please consult the how-to pages.</p>"},{"location":"#limitations-to-streaming","title":"Limitations to streaming","text":"<p>The streaming algorithm exploits the fact that convolutions are locally defined. This means that the entire input image does not have to be parsed through the network at once, but can be reconstructed piece-wise. There are many layers that do not possess this property, such as batch normalization layers, where the mean and standard deviations computations require the entire image at once.</p>"},{"location":"#layers-that-can-be-used-without-issue","title":"Layers that can be used without issue","text":"<ul> <li>Convolutional layers: can be used without issue, since they are locally defined</li> <li>Pooling layers such as average, max, GEM pooling, as long as they are locally defined, e.g. a 2x2 kernel. Global pooling will not work.</li> <li>Any other layer that is defined locally and not dependent on seeing the entire image at once. </li> </ul>"},{"location":"#layers-that-are-restricted","title":"Layers that are restricted","text":"<ul> <li>All normalization layers: e.g. batch normalization. Most normalization layers require image-level statistics such as means and standard deviations to be computed. As streaming works tile-wise, they will not yield the correct results. Therefore, all normalization layers must be set to <code>eval()</code> during training.</li> <li>Dense layers can only be used to model 1x1 convolutions, i.e. a fully connected layer that works over the channels of an input, rather than the spatial dimensions.</li> </ul>"},{"location":"#references","title":"References","text":"<p>[1]  H. Pinckaers, B. van Ginneken and G. Litjens, \"Streaming convolutional neural networks for end-to-end learning with multi-megapixel images,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence,  doi: 10.1109/TPAMI.2020.3019563</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installing-lightstream","title":"Installing lightstream","text":"<p>Installing the lightstream package can be done via pip: </p><pre><code>pip install lightstream\n</code></pre> <p>Alternative, the project can be cloned using <code>git</code> from the lightstream git repo.  From there, the package can again be installed using <code>pip install .</code> within the directory. Otherwise, you can use the repository as-is, but you will have to add it to your <code>PYTHONPATH</code> if you want to access it from other repositories.</p>"},{"location":"getting_started/installation/#installing-pyvips","title":"Installing (Py)vips","text":"<p>Before pyvips can be installed, libvips must be present on the system, as pyvips is a Python wrapper around the libvips library. Libvips can be installed either using the binaries on the libvips website, or it can be built from source with or without Docker. After libvips is properly installed, pyvips can be installed via pip: </p><pre><code>pip install pyvips\n</code></pre>"},{"location":"getting_started/installation/#installing-torch-related-libraries","title":"Installing torch-related libraries","text":"<p>Installing torch and its related libraries (lightning, torchvision) can be done via pip installs. We recommend looking at their respective websites for further help to install these packages.</p>"},{"location":"getting_started/requirements/","title":"Requirements","text":""},{"location":"getting_started/requirements/#software-requirements","title":"Software Requirements","text":"<p>The lightstream package is supported on Linux distributions. Although it should work on Windows, it is untested and therefore not recommended.</p> <p>The following packages will need to be installed for lightstream to work. Note that albumentationsXL is not strictly required, but features an albumentations-like data augmentations pipeline for processing large images.</p> <ul> <li>Pytorch (2.0.0) or higher</li> <li>Pytorch-lightning (2.0.0) or higher</li> <li>torchvision (0.15 or higher)</li> <li>Pyvips</li> <li>Requires libvips to be installed and configured.</li> <li>albumentationsXL (optional, but recommended)</li> </ul>"},{"location":"getting_started/requirements/#hardware-requirements","title":"Hardware requirements","text":"<ul> <li> <p>GPU: Considering lightstream is computationally heavy, it is highly recommend to use a GPU with at least 10GB VRAM instead of CPU backends.</p> </li> <li> <p>RAM: Additionally, large amounts of RAM are highly recommended (at least 32 GB). Although pyvips will keep memory footprints low during execution, they will have to be stored in RAM before sending it over to the GPU. Since we are dealing with large images, this can become quite costly. To give an idea of the RAM cost, we have included a reference table of the approximate memory footprint of several image sizes with varying dtypes. Given the values in table 1, it is recommended to work with np.uint8 as much as possible, and only use float32 sparingly.  Float64 images are usually unnecessary and should be avoided entirely.</p> </li> </ul> Table 1: All values are in gigabyes (GB) Image size uint8 float16 float32 float64 8192x8192x3 0.2 0.4 0.8 1.6 16384x16384x3 0.8 1.6 3.2 6.4 32768x32768x3 3.2 6.4 12.8 25.6 65536x65536x3 12.8 25.6 51.2 102.4"},{"location":"models/convnext/","title":"Resnet","text":"<p>             Bases: <code>ImageNetClassifier</code></p> Source code in <code>lightstream\\models\\convnext\\convnext.py</code> <pre><code>class StreamingConvnext(ImageNetClassifier):\n    model_choices = {\"convnext_tiny\": convnext_tiny, \"convnext_small\": convnext_small}\n\n    def __init__(\n        self,\n        model_name: str,\n        tile_size: int,\n        loss_fn: torch.nn.functional,\n        train_streaming_layers: bool = True,\n        use_stochastic_depth: bool = False,\n        metrics: MetricCollection | None = None,\n        **kwargs,\n    ):\n        assert model_name in list(StreamingConvnext.model_choices.keys())\n\n        self.model_name = model_name\n        self.use_stochastic_depth = use_stochastic_depth\n\n        network = StreamingConvnext.model_choices[model_name](weights=\"DEFAULT\")\n        stream_network, head = network.features, torch.nn.Sequential(network.avgpool, network.classifier)\n        self._get_streaming_options(**kwargs)\n\n        super().__init__(\n            stream_network,\n            head,\n            tile_size,\n            loss_fn,\n            train_streaming_layers=train_streaming_layers,\n            metrics=metrics\n            **self.streaming_options,\n        )\n\n        # By default, the after_streaming_init callback turns sd off\n        _toggle_stochastic_depth(self.stream_network.stream_module, training=self.use_stochastic_depth)\n\n    def _get_streaming_options(self, **kwargs):\n        \"\"\"Set streaming defaults, but overwrite them with values of kwargs if present.\"\"\"\n\n        streaming_options = {\n            \"verbose\": True,\n            \"copy_to_gpu\": False,\n            \"statistics_on_cpu\": True,\n            \"normalize_on_gpu\": True,\n            \"mean\": [0.485, 0.456, 0.406],\n            \"std\": [0.229, 0.224, 0.225],\n            \"before_streaming_init_callbacks\": [_set_layer_scale],\n            \"after_streaming_init_callbacks\": [_toggle_stochastic_depth]\n        }\n        self.streaming_options = {**streaming_options, **kwargs}\n</code></pre>"},{"location":"models/resnet/","title":"Resnet","text":"<p>             Bases: <code>ImageNetClassifier</code></p> Source code in <code>lightstream\\models\\resnet\\resnet.py</code> <pre><code>class StreamingResNet(ImageNetClassifier):\n    # Resnet  minimal tile size based on tile statistics calculations:\n    # resnet18 : 960\n\n    model_choices = {\"resnet18\": resnet18, \"resnet34\": resnet34, \"resnet50\": resnet50}\n\n    def __init__(\n        self,\n        model_name: str,\n        tile_size: int,\n        loss_fn: torch.nn.functional,\n        train_streaming_layers: bool = True,\n        metrics: MetricCollection | None = None,\n        **kwargs\n    ):\n        assert model_name in list(StreamingResNet.model_choices.keys())\n        network = StreamingResNet.model_choices[model_name](weights=\"DEFAULT\")\n        stream_network, head = split_resnet(network, num_classes=kwargs.pop(\"num_classes\", 1000))\n\n        self._get_streaming_options(**kwargs)\n        print(\"metrics\", metrics)\n        super().__init__(\n            stream_network,\n            head,\n            tile_size,\n            loss_fn,\n            train_streaming_layers=train_streaming_layers,\n            metrics=metrics,\n            **self.streaming_options,\n        )\n\n    def _get_streaming_options(self, **kwargs):\n        \"\"\"Set streaming defaults, but overwrite them with values of kwargs if present.\"\"\"\n\n        # We need to add torch.nn.Batchnorm to the keep modules, because of some in-place ops error if we don't\n        # https://discuss.pytorch.org/t/register-full-backward-hook-for-residual-connection/146850\n        streaming_options = {\n            \"verbose\": True,\n            \"copy_to_gpu\": False,\n            \"statistics_on_cpu\": True,\n            \"normalize_on_gpu\": True,\n            \"mean\": [0.485, 0.456, 0.406],\n            \"std\": [0.229, 0.224, 0.225],\n            \"add_keep_modules\": [torch.nn.BatchNorm2d],\n        }\n        self.streaming_options = {**streaming_options, **kwargs}\n</code></pre>"},{"location":"modules/constructor/","title":"StreamingModule","text":"Source code in <code>lightstream\\modules\\constructor.py</code> <pre><code>class StreamingConstructor:\n    def __init__(\n        self,\n        model: torch.nn.modules,\n        tile_size: int,\n        verbose: bool = False,\n        deterministic: bool = False,\n        saliency: bool = False,\n        copy_to_gpu: bool = False,\n        statistics_on_cpu: bool = False,\n        normalize_on_gpu: bool = False,\n        mean: list[float, float, float] | None = None,\n        std: list[float, float, float] | None = None,\n        tile_cache: dict | None = None,\n        add_keep_modules: list[torch.nn.modules] | None = None,\n        before_streaming_init_callbacks: list[Callable[[torch.nn.modules], None], ...] | None = None,\n        after_streaming_init_callbacks: list[Callable[[torch.nn.modules], None], ...] | None = None,\n    ):\n        self.model = model\n        self.model_copy = deepcopy(self.model)\n        self.state_dict = self.save_parameters()\n\n        self.tile_size = tile_size\n        self.verbose = verbose\n        self.deterministic = deterministic\n        self.saliency = saliency\n        self.copy_to_gpu = copy_to_gpu\n        self.statistics_on_cpu = statistics_on_cpu\n        self.normalize_on_gpu = normalize_on_gpu\n        self.mean = mean\n        self.std = std\n        self.tile_cache = tile_cache\n\n        self.before_streaming_init_callbacks = before_streaming_init_callbacks or []\n        self.after_streaming_init_callbacks = after_streaming_init_callbacks or []\n\n        self.keep_modules = [\n            torch.nn.Conv1d,\n            torch.nn.Conv2d,\n            torch.nn.Conv3d,\n            torch.nn.AvgPool1d,\n            torch.nn.AvgPool2d,\n            torch.nn.AvgPool3d,\n            torch.nn.MaxPool1d,\n            torch.nn.MaxPool2d,\n            torch.nn.MaxPool3d,\n        ]\n\n        if add_keep_modules is not None:\n            self.add_modules_to_keep(add_keep_modules)\n\n        if not self.statistics_on_cpu:\n            # Move to cuda manually if statistics are computed on gpu\n            device = torch.device(\"cuda\")\n            self.model.to(device)\n\n    def add_modules_to_keep(self, module_list: list):\n        \"\"\"Add extra layers to keep during streaming tile calculations\n\n        Modules in the keep_modules list will not be set to nn.Identity() during streaming initialization\n        Parameters\n        ----------\n        module_list : list\n            A list of torch modules to add to the keep_modules list.\n        \"\"\"\n\n        self.keep_modules.extend(module_list)\n\n    def prepare_streaming_model(self):\n        \"\"\"Run pre and postprocessing for tile lost calculations\n        Returns\n        -------\n        sCNN : torch.nn.modules\n            The streaming module\n        \"\"\"\n\n        # If tile cache is available, it has already been initialized successfully once\n        if self.tile_cache:\n            return self.create_streaming_model()\n\n        print(\"\")\n        # Prepare for streaming tile statistics calculations\n        print(\"Converting modules to nn.Identity()\")\n        self.convert_to_identity(self.model)\n        # execute any callbacks that further preprocess the model\n        print(\"Executing pre-streaming initialization callbacks (if any):\")\n        self._execute_before_callbacks()\n\n        print(\"Initializing streaming model\")\n        sCNN = self.create_streaming_model()\n\n        # check self.stream_network, and reload the proper weights\n        print(\"Restoring model weights\")\n        self.restore_model_layers(self.model_copy, sCNN.stream_module)\n        sCNN.stream_module.load_state_dict(self.state_dict)\n\n        print(\"Executing post-streaming initialization callbacks (if any):\")\n        self._execute_after_callbacks()\n        return sCNN\n\n    def _execute_before_callbacks(self):\n        for cb_func in self.before_streaming_init_callbacks:\n            print(f\"Executing callback function {cb_func}\")\n            cb_func(self.model)\n        print(\"\")\n\n    def _execute_after_callbacks(self):\n        for cb_func in self.after_streaming_init_callbacks:\n            print(f\"Executing callback function {cb_func}\")\n            cb_func(self.model)\n        print(\"\")\n\n    def create_streaming_model(self):\n        return StreamingCNN(\n            self.model,\n            tile_shape=(1, 3, self.tile_size, self.tile_size),\n            deterministic=self.deterministic,\n            saliency=self.saliency,\n            copy_to_gpu=self.copy_to_gpu,\n            verbose=self.verbose,\n            statistics_on_cpu=self.statistics_on_cpu,\n            normalize_on_gpu=self.normalize_on_gpu,\n            mean=self.mean,\n            std=self.std,\n            state_dict=self.tile_cache,\n        )\n\n    def save_parameters(self):\n        state_dict = self.model.state_dict()\n        state_dict = deepcopy(state_dict)\n        return state_dict\n\n    def convert_to_identity(self, model: torch.nn.modules):\n        \"\"\"Convert non-conv and non-local pooling layers to identity\n\n        Parameters\n        ----------\n        model : torch.nn.Sequential\n            The model to substitute\n        \"\"\"\n\n        for n, module in model.named_children():\n            if len(list(module.children())) &gt; 0:\n                # compound module, go inside it\n                self.convert_to_identity(module)\n                continue\n\n            # if new module is assigned to a variable, e.g. new = nn.Identity(), then it's considered a duplicate in\n            # module.named_children used later. Instead, we use in-place assignment, so each new module is unique\n            if not isinstance(module, tuple(self.keep_modules)):\n                try:\n                    n = int(n)\n                    model[n] = torch.nn.Identity()\n                except ValueError:\n                    setattr(model, str(n), torch.nn.Identity())\n\n    def restore_model_layers(self, model_ref, model_rep):\n        \"\"\"Restore model layers from Identity to what they were before\n\n        This function requires an exact copy of the model (model_ref) before its layers were set to nn.Identity()\n        (model_rep)\n\n        Parameters\n        ----------\n        model_ref : torch.nn.modules\n            The copy of the model before it was set to nn.Identity()\n        model_rep : torch.nn.modules\n            The stream_module attribute within the streaming model that were set to nn.Identity\n        \"\"\"\n\n        for ref, rep in zip(model_ref.named_children(), model_rep.named_children()):\n            n_ref, module_ref = ref\n            n_rep, module_rep = rep\n\n            if len(list(module_ref.children())) &gt; 0:\n                # compound module, go inside it\n                self.restore_model_layers(module_ref, module_rep)\n                continue\n\n            if isinstance(module_rep, torch.nn.Identity):\n                # simple module\n                try:\n                    n_ref = int(n_ref)\n                    model_rep[n_rep] = model_ref[n_ref]\n                except (ValueError, TypeError):\n                    try:\n                        setattr(model_rep, n_rep, model_ref[int(n_ref)])\n                    except (ValueError, TypeError):\n                        # Try setting it through block dot operations\n                        setattr(model_rep, n_rep, getattr(model_ref, n_ref))\n</code></pre>"},{"location":"modules/constructor/#modules.constructor.StreamingConstructor.add_modules_to_keep","title":"<code>add_modules_to_keep(module_list)</code>","text":"<p>Add extra layers to keep during streaming tile calculations</p> <p>Modules in the keep_modules list will not be set to nn.Identity() during streaming initialization</p> <p>Parameters:</p> Name Type Description Default <code>module_list</code> <code>list</code> <p>A list of torch modules to add to the keep_modules list.</p> required Source code in <code>lightstream\\modules\\constructor.py</code> <pre><code>def add_modules_to_keep(self, module_list: list):\n    \"\"\"Add extra layers to keep during streaming tile calculations\n\n    Modules in the keep_modules list will not be set to nn.Identity() during streaming initialization\n    Parameters\n    ----------\n    module_list : list\n        A list of torch modules to add to the keep_modules list.\n    \"\"\"\n\n    self.keep_modules.extend(module_list)\n</code></pre>"},{"location":"modules/constructor/#modules.constructor.StreamingConstructor.convert_to_identity","title":"<code>convert_to_identity(model)</code>","text":"<p>Convert non-conv and non-local pooling layers to identity</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The model to substitute</p> required Source code in <code>lightstream\\modules\\constructor.py</code> <pre><code>def convert_to_identity(self, model: torch.nn.modules):\n    \"\"\"Convert non-conv and non-local pooling layers to identity\n\n    Parameters\n    ----------\n    model : torch.nn.Sequential\n        The model to substitute\n    \"\"\"\n\n    for n, module in model.named_children():\n        if len(list(module.children())) &gt; 0:\n            # compound module, go inside it\n            self.convert_to_identity(module)\n            continue\n\n        # if new module is assigned to a variable, e.g. new = nn.Identity(), then it's considered a duplicate in\n        # module.named_children used later. Instead, we use in-place assignment, so each new module is unique\n        if not isinstance(module, tuple(self.keep_modules)):\n            try:\n                n = int(n)\n                model[n] = torch.nn.Identity()\n            except ValueError:\n                setattr(model, str(n), torch.nn.Identity())\n</code></pre>"},{"location":"modules/constructor/#modules.constructor.StreamingConstructor.prepare_streaming_model","title":"<code>prepare_streaming_model()</code>","text":"<p>Run pre and postprocessing for tile lost calculations</p> <p>Returns:</p> Name Type Description <code>sCNN</code> <code>modules</code> <p>The streaming module</p> Source code in <code>lightstream\\modules\\constructor.py</code> <pre><code>def prepare_streaming_model(self):\n    \"\"\"Run pre and postprocessing for tile lost calculations\n    Returns\n    -------\n    sCNN : torch.nn.modules\n        The streaming module\n    \"\"\"\n\n    # If tile cache is available, it has already been initialized successfully once\n    if self.tile_cache:\n        return self.create_streaming_model()\n\n    print(\"\")\n    # Prepare for streaming tile statistics calculations\n    print(\"Converting modules to nn.Identity()\")\n    self.convert_to_identity(self.model)\n    # execute any callbacks that further preprocess the model\n    print(\"Executing pre-streaming initialization callbacks (if any):\")\n    self._execute_before_callbacks()\n\n    print(\"Initializing streaming model\")\n    sCNN = self.create_streaming_model()\n\n    # check self.stream_network, and reload the proper weights\n    print(\"Restoring model weights\")\n    self.restore_model_layers(self.model_copy, sCNN.stream_module)\n    sCNN.stream_module.load_state_dict(self.state_dict)\n\n    print(\"Executing post-streaming initialization callbacks (if any):\")\n    self._execute_after_callbacks()\n    return sCNN\n</code></pre>"},{"location":"modules/constructor/#modules.constructor.StreamingConstructor.restore_model_layers","title":"<code>restore_model_layers(model_ref, model_rep)</code>","text":"<p>Restore model layers from Identity to what they were before</p> <p>This function requires an exact copy of the model (model_ref) before its layers were set to nn.Identity() (model_rep)</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>modules</code> <p>The copy of the model before it was set to nn.Identity()</p> required <code>model_rep</code> <code>modules</code> <p>The stream_module attribute within the streaming model that were set to nn.Identity</p> required Source code in <code>lightstream\\modules\\constructor.py</code> <pre><code>def restore_model_layers(self, model_ref, model_rep):\n    \"\"\"Restore model layers from Identity to what they were before\n\n    This function requires an exact copy of the model (model_ref) before its layers were set to nn.Identity()\n    (model_rep)\n\n    Parameters\n    ----------\n    model_ref : torch.nn.modules\n        The copy of the model before it was set to nn.Identity()\n    model_rep : torch.nn.modules\n        The stream_module attribute within the streaming model that were set to nn.Identity\n    \"\"\"\n\n    for ref, rep in zip(model_ref.named_children(), model_rep.named_children()):\n        n_ref, module_ref = ref\n        n_rep, module_rep = rep\n\n        if len(list(module_ref.children())) &gt; 0:\n            # compound module, go inside it\n            self.restore_model_layers(module_ref, module_rep)\n            continue\n\n        if isinstance(module_rep, torch.nn.Identity):\n            # simple module\n            try:\n                n_ref = int(n_ref)\n                model_rep[n_rep] = model_ref[n_ref]\n            except (ValueError, TypeError):\n                try:\n                    setattr(model_rep, n_rep, model_ref[int(n_ref)])\n                except (ValueError, TypeError):\n                    # Try setting it through block dot operations\n                    setattr(model_rep, n_rep, getattr(model_ref, n_ref))\n</code></pre>"},{"location":"modules/imagenettemplate/","title":"BaseModule","text":"<p>             Bases: <code>StreamingModule</code></p> Source code in <code>lightstream\\modules\\imagenet_template.py</code> <pre><code>class ImageNetClassifier(StreamingModule):\n    def __init__(\n        self,\n        stream_net: torch.nn.modules.container.Sequential,\n        head: torch.nn.modules.container.Sequential,\n        tile_size: int,\n        loss_fn: torch.nn.modules.loss,\n        train_streaming_layers=True,\n        metrics: MetricCollection | None = None,\n        **kwargs\n    ):\n        super().__init__(stream_net, tile_size, train_streaming_layers, **kwargs)\n        self.head = head\n        self.loss_fn = loss_fn\n        self.train_streaming_layers = train_streaming_layers\n        self.params = self.extend_trainable_params()\n\n        self.train_metrics = metrics.clone(prefix='train_') if metrics else None\n        self.val_metrics = metrics.clone(prefix='val_') if metrics else None\n        self.test_metrics = metrics.clone(prefix='test_') if metrics else None\n\n    def on_train_epoch_start(self) -&gt; None:\n        super().on_train_epoch_start()\n\n    def forward_head(self, x):\n        return self.head(x)\n\n    def forward(self, x):\n        fmap = self.forward_streaming(x)\n        out = self.forward_head(fmap)\n        return out\n\n    def training_step(self, batch: Any, batch_idx: int, *args: Any, **kwargs: Any) -&gt; tuple[Any, Any, Any]:\n        image, target = batch\n        self.image = image\n\n        self.str_output = self.forward_streaming(image)\n\n        # let leaf tensor require grad when training with streaming\n        self.str_output.requires_grad = self.training\n\n        logits = self.forward_head(self.str_output)\n\n        loss = self.loss_fn(logits, target)\n\n        output = {}\n        if self.train_metrics:\n            output = self.train_metrics(logits, target)\n\n        output[\"train_loss\"] = loss\n\n        self.log_dict(output, prog_bar=True, on_step=True,  on_epoch=True, sync_dist=True,)\n        return loss\n\n    def validation_step(self, batch: Any, batch_idx: int, *args: Any, **kwargs: Any) -&gt; STEP_OUTPUT:\n        image, target = batch\n        self.image = image\n\n        self.str_output = self.forward_streaming(image)\n\n        # let leaf tensor require grad when training with streaming\n        self.str_output.requires_grad = self.training\n\n        logits = self.forward_head(self.str_output)\n\n        loss = self.loss_fn(logits, target)\n\n        output = {}\n        if self.val_metrics:\n            output = self.train_metrics(logits, target)\n\n        output[\"val_loss\"] = loss\n\n        self.log_dict(output, prog_bar=True, on_step=False,  on_epoch=True, sync_dist=True,)\n\n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.params, lr=1e-3)\n        return opt\n\n    def extend_trainable_params(self):\n        if self.params:\n            return self.params + list(self.head.parameters())\n        return list(self.head.parameters())\n\n    def backward(self, loss: Tensor, **kwargs) -&gt; None:\n        loss.backward()\n        # del loss\n        # Don't call this&gt;? https://pytorch-lightning.readthedocs.io/en/1.5.10/guides/speed.html#things-to-avoid\n        torch.cuda.empty_cache()\n        if self.train_streaming_layers:\n            self.backward_streaming(self.image, self.str_output.grad)\n        del self.str_output\n</code></pre>"},{"location":"modules/streamingmodule/","title":"StreamingModule","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>class StreamingModule(L.LightningModule):\n    def __init__(self, stream_network, tile_size, train_streaming_layers=True, **kwargs):\n        super().__init__()\n        self.train_streaming_layers = train_streaming_layers\n        # self._stream_module = stream_network\n\n        # StreamingCNN options\n        self._tile_size = tile_size\n        self.tile_cache_dir = kwargs.pop(\"tile_cache_dir\", Path.cwd())\n        self.tile_cache_fname = kwargs.pop(\"tile_cache_fname\", None)\n\n        # Load the tile cache state dict if present\n        tile_cache = self.load_tile_cache_if_needed()\n\n        # Initialize the streaming network\n        self._constructor_opts = kwargs\n        self.constructor = StreamingConstructor(\n            stream_network, self.tile_size, tile_cache=tile_cache, **self._constructor_opts\n        )\n        self.copy_to_gpu = self.constructor.copy_to_gpu\n        self.stream_network = self.constructor.prepare_streaming_model()\n\n        self.save_tile_cache_if_needed()\n        self.params = self.get_trainable_params()\n\n    @property\n    def tile_size(self):\n        return self._tile_size\n\n    @tile_size.setter\n    def tile_size(self, new_tile_size):\n        self._tile_size = new_tile_size\n\n    def freeze_streaming_normalization_layers(self):\n        \"\"\"Do not use normalization layers within lightstream, only local ops are allowed\"\"\"\n        freeze_layers = [\n            l\n            for l in self.stream_network.stream_module.modules()\n            if isinstance(l, (torch.nn.BatchNorm2d, torch.nn.LayerNorm))\n        ]\n\n        for mod in freeze_layers:\n            mod.eval()\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"on_train_epoch_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.freeze_streaming_normalization_layers()\n\n    def prepare_start_for_streaming(self):\n\n        # Update streaming to put all the inputs/tensors on the right device\n        self.stream_network.device = self.device\n        self.stream_network.mean = self.stream_network.mean.to(self.device, non_blocking=True)\n        self.stream_network.std = self.stream_network.std.to(self.device, non_blocking=True)\n        if self.trainer.precision == \"16-mixed\":\n            self.stream_network.dtype = torch.float16\n        elif self.trainer.precision == \"bf16-mixed\":\n            self.stream_network.dtype = torch.float16\n        else:\n            self.stream_network.dtype = self.dtype\n\n    def on_validation_start(self):\n        \"\"\"on_validation_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.prepare_start_for_streaming()\n\n    def on_train_start(self):\n        \"\"\"on_train_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.prepare_start_for_streaming()\n\n\n    def on_test_start(self):\n        \"\"\"on_test_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.prepare_start_for_streaming()\n\n\n    def on_predict_start(self):\n        \"\"\"on_predict_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.prepare_start_for_streaming()\n\n\n    def disable_streaming_hooks(self):\n        \"\"\"Disable streaming hooks and replace streamingconv2d  with conv2d modules\n\n        This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient\n        checkpointing will be turned off.\n        \"\"\"\n        self.stream_network.disable()\n\n    def enable_streaming_hooks(self):\n        \"\"\"Enable streaming hooks and use streamingconv2d modules\"\"\"\n        self.stream_network.enable()\n\n    def forward_streaming(self, x):\n        \"\"\"\n\n        Parameters\n        ----------\n        x : torch.Tensor\n        The input tensor in [1,C,H,W] format\n\n        Returns\n        -------\n        out: torch.Tensor\n        The output of the streaming model\n\n        \"\"\"\n        return self.stream_network.forward(x)\n\n    def backward_streaming(self, image, gradient):\n        \"\"\"Perform the backward pass using the streaming network\n\n        Backward only if streaming is turned on.\n        This method is primarily a convenience function\n\n        Parameters\n        ----------\n        image: torch.Tensor\n            The input image in [1,C,H,W] format\n        gradient: torch.Tensor\n            The gradient of the next layer in the model to continue backpropagation with\n\n        Returns\n        -------\n\n        \"\"\"\n\n        # If requires_grad is set to false, .backward() in streaming causes errors or overhead, so use a bool\n        if self.train_streaming_layers:\n            self.stream_network.backward(image, gradient)\n\n    def training_step(self, *args: Any, **kwargs: Any) -&gt; STEP_OUTPUT:\n        raise NotImplementedError\n\n    def backward(self, loss: Tensor, *args: Any, **kwargs: Any) -&gt; None:\n        raise NotImplementedError\n\n    def configure_tile_stride(self):\n        \"\"\"\n        Helper function that returns the tile stride during streaming.\n\n        Streaming assumes that the input image is perfectly divisible with the network output stride or the\n        tile stride. This function will return the tile stride, which can then be used within data processing pipelines\n        to pad/crop images to a multiple of the tile stride.\n\n        Examples:\n\n        Returns\n        -------\n        tile_stride: numpy.ndarray\n            the tile stride.\n\n\n        \"\"\"\n        stride = self.tile_size - (\n            self.stream_network.tile_gradient_lost.left + self.stream_network.tile_gradient_lost.right\n        )\n        stride = stride // self.stream_network.output_stride[-1]\n        stride *= self.stream_network.output_stride[-1]\n        return stride.detach().cpu().numpy()\n\n    def get_trainable_params(self):\n        \"\"\"Get trainable parameters for the entire model\n\n        If self.streaming_layers is True, then the parameters of the streaming network will be trained.\n        Otherwise, the parameters will be left untrained (no gradients will be collected)\n\n        \"\"\"\n\n        if self.train_streaming_layers:\n            params = list(self.stream_network.stream_module.parameters())\n            return params\n        else:\n            print(\"WARNING: Streaming network will not be trained\")\n            for param in self.stream_network.stream_module.parameters():\n                param.requires_grad = False\n\n\n    def _remove_streaming_network(self):\n        \"\"\"Converts the streaming network into a non-streaming network\n\n        The former streaming encoder can be addressed as self.stream_network\n        This function is currently untested and breaks the class, since there is no way to rebuild the streaming network\n        other than calling a new class directly.\n\n        \"\"\"\n\n        # Convert streamingConv2D into regular Conv2D and turn off streaming hooks\n        self.disable_streaming_hooks()\n        temp = self.stream_network.stream_module\n\n        # torch modules cannot be overridden normally, so delete and reassign\n        del self.stream_network\n        self.stream_network = temp\n\n    def save_tile_cache_if_needed(self, overwrite=False):\n        \"\"\"\n        Writes the tile cache to a file, so it does not have to be recomputed\n\n        The tile cache is normally calculated for each run.\n        However, this can take a long time. By writing it to a file it can be reloaded without the need\n        for recomputation.\n\n        Limitations:\n        This only works for the exact same model and for a single tile size. If the streaming part of the model\n        changes, or if the tile size is changed, it will no longer work.\n\n        \"\"\"\n        if self.tile_cache_fname is None:\n            self.tile_cache_fname = \"tile_cache_\" + \"1_3_\" + str(self.tile_size) + \"_\" + str(self.tile_size)\n        write_path = Path(self.tile_cache_dir) / Path(self.tile_cache_fname)\n\n        if Path(self.tile_cache_dir).exists():\n            if write_path.exists() and not overwrite:\n                print(\"previous tile cache found and overwrite is false, not saving\")\n\n            elif self.global_rank == 0:\n                print(f\"writing streaming cache file to {str(write_path)}\")\n                torch.save(self.stream_network.get_tile_cache(), str(write_path))\n\n            else:\n                print(\"\")\n        else:\n            raise NotADirectoryError(f\"Did not find {self.tile_cache_dir} or does not exist\")\n\n    def load_tile_cache_if_needed(self, use_tile_cache: bool = True):\n        \"\"\"\n        Load the tile cache for the model from the read_dir\n\n        Parameters\n        ----------\n        use_tile_cache : bool\n            Whether to use the tile cache file and load it into the streaming module\n\n        Returns\n        ---------\n        state_dict : torch.state_dict | None\n            The state dict if present\n        \"\"\"\n\n        if self.tile_cache_fname is None:\n            self.tile_cache_fname = \"tile_cache_\" + \"1_3_\" + str(self.tile_size) + \"_\" + str(self.tile_size)\n\n        tile_cache_loc = Path(self.tile_cache_dir) / Path(self.tile_cache_fname)\n\n        if tile_cache_loc.exists() and use_tile_cache:\n            print(\"Loading tile cache from\", tile_cache_loc)\n            state_dict = torch.load(str(tile_cache_loc), map_location=lambda storage, loc: storage)\n        else:\n            print(\"No tile cache found, calculating it now\")\n            state_dict = None\n\n        return state_dict\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.backward_streaming","title":"<code>backward_streaming(image, gradient)</code>","text":"<p>Perform the backward pass using the streaming network</p> <p>Backward only if streaming is turned on. This method is primarily a convenience function</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>The input image in [1,C,H,W] format</p> required <code>gradient</code> <p>The gradient of the next layer in the model to continue backpropagation with</p> required Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def backward_streaming(self, image, gradient):\n    \"\"\"Perform the backward pass using the streaming network\n\n    Backward only if streaming is turned on.\n    This method is primarily a convenience function\n\n    Parameters\n    ----------\n    image: torch.Tensor\n        The input image in [1,C,H,W] format\n    gradient: torch.Tensor\n        The gradient of the next layer in the model to continue backpropagation with\n\n    Returns\n    -------\n\n    \"\"\"\n\n    # If requires_grad is set to false, .backward() in streaming causes errors or overhead, so use a bool\n    if self.train_streaming_layers:\n        self.stream_network.backward(image, gradient)\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.configure_tile_stride","title":"<code>configure_tile_stride()</code>","text":"<p>Helper function that returns the tile stride during streaming.</p> <p>Streaming assumes that the input image is perfectly divisible with the network output stride or the tile stride. This function will return the tile stride, which can then be used within data processing pipelines to pad/crop images to a multiple of the tile stride.</p> <p>Examples:</p> <p>Returns:</p> Name Type Description <code>tile_stride</code> <code>ndarray</code> <p>the tile stride.</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def configure_tile_stride(self):\n    \"\"\"\n    Helper function that returns the tile stride during streaming.\n\n    Streaming assumes that the input image is perfectly divisible with the network output stride or the\n    tile stride. This function will return the tile stride, which can then be used within data processing pipelines\n    to pad/crop images to a multiple of the tile stride.\n\n    Examples:\n\n    Returns\n    -------\n    tile_stride: numpy.ndarray\n        the tile stride.\n\n\n    \"\"\"\n    stride = self.tile_size - (\n        self.stream_network.tile_gradient_lost.left + self.stream_network.tile_gradient_lost.right\n    )\n    stride = stride // self.stream_network.output_stride[-1]\n    stride *= self.stream_network.output_stride[-1]\n    return stride.detach().cpu().numpy()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.disable_streaming_hooks","title":"<code>disable_streaming_hooks()</code>","text":"<p>Disable streaming hooks and replace streamingconv2d  with conv2d modules</p> <p>This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient checkpointing will be turned off.</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def disable_streaming_hooks(self):\n    \"\"\"Disable streaming hooks and replace streamingconv2d  with conv2d modules\n\n    This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient\n    checkpointing will be turned off.\n    \"\"\"\n    self.stream_network.disable()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.enable_streaming_hooks","title":"<code>enable_streaming_hooks()</code>","text":"<p>Enable streaming hooks and use streamingconv2d modules</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def enable_streaming_hooks(self):\n    \"\"\"Enable streaming hooks and use streamingconv2d modules\"\"\"\n    self.stream_network.enable()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.forward_streaming","title":"<code>forward_streaming(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> required <code>The</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <code>The output of the streaming model</code> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def forward_streaming(self, x):\n    \"\"\"\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    The input tensor in [1,C,H,W] format\n\n    Returns\n    -------\n    out: torch.Tensor\n    The output of the streaming model\n\n    \"\"\"\n    return self.stream_network.forward(x)\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.freeze_streaming_normalization_layers","title":"<code>freeze_streaming_normalization_layers()</code>","text":"<p>Do not use normalization layers within lightstream, only local ops are allowed</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def freeze_streaming_normalization_layers(self):\n    \"\"\"Do not use normalization layers within lightstream, only local ops are allowed\"\"\"\n    freeze_layers = [\n        l\n        for l in self.stream_network.stream_module.modules()\n        if isinstance(l, (torch.nn.BatchNorm2d, torch.nn.LayerNorm))\n    ]\n\n    for mod in freeze_layers:\n        mod.eval()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.get_trainable_params","title":"<code>get_trainable_params()</code>","text":"<p>Get trainable parameters for the entire model</p> <p>If self.streaming_layers is True, then the parameters of the streaming network will be trained. Otherwise, the parameters will be left untrained (no gradients will be collected)</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def get_trainable_params(self):\n    \"\"\"Get trainable parameters for the entire model\n\n    If self.streaming_layers is True, then the parameters of the streaming network will be trained.\n    Otherwise, the parameters will be left untrained (no gradients will be collected)\n\n    \"\"\"\n\n    if self.train_streaming_layers:\n        params = list(self.stream_network.stream_module.parameters())\n        return params\n    else:\n        print(\"WARNING: Streaming network will not be trained\")\n        for param in self.stream_network.stream_module.parameters():\n            param.requires_grad = False\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.load_tile_cache_if_needed","title":"<code>load_tile_cache_if_needed(use_tile_cache=True)</code>","text":"<p>Load the tile cache for the model from the read_dir</p> <p>Parameters:</p> Name Type Description Default <code>use_tile_cache</code> <code>bool</code> <p>Whether to use the tile cache file and load it into the streaming module</p> <code>True</code> <p>Returns:</p> Name Type Description <code>state_dict</code> <code>state_dict | None</code> <p>The state dict if present</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def load_tile_cache_if_needed(self, use_tile_cache: bool = True):\n    \"\"\"\n    Load the tile cache for the model from the read_dir\n\n    Parameters\n    ----------\n    use_tile_cache : bool\n        Whether to use the tile cache file and load it into the streaming module\n\n    Returns\n    ---------\n    state_dict : torch.state_dict | None\n        The state dict if present\n    \"\"\"\n\n    if self.tile_cache_fname is None:\n        self.tile_cache_fname = \"tile_cache_\" + \"1_3_\" + str(self.tile_size) + \"_\" + str(self.tile_size)\n\n    tile_cache_loc = Path(self.tile_cache_dir) / Path(self.tile_cache_fname)\n\n    if tile_cache_loc.exists() and use_tile_cache:\n        print(\"Loading tile cache from\", tile_cache_loc)\n        state_dict = torch.load(str(tile_cache_loc), map_location=lambda storage, loc: storage)\n    else:\n        print(\"No tile cache found, calculating it now\")\n        state_dict = None\n\n    return state_dict\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_predict_start","title":"<code>on_predict_start()</code>","text":"<p>on_predict_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_predict_start(self):\n    \"\"\"on_predict_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.prepare_start_for_streaming()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_test_start","title":"<code>on_test_start()</code>","text":"<p>on_test_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_test_start(self):\n    \"\"\"on_test_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.prepare_start_for_streaming()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>on_train_epoch_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"on_train_epoch_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.freeze_streaming_normalization_layers()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_train_start","title":"<code>on_train_start()</code>","text":"<p>on_train_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_train_start(self):\n    \"\"\"on_train_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.prepare_start_for_streaming()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_validation_start","title":"<code>on_validation_start()</code>","text":"<p>on_validation_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_validation_start(self):\n    \"\"\"on_validation_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.prepare_start_for_streaming()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.save_tile_cache_if_needed","title":"<code>save_tile_cache_if_needed(overwrite=False)</code>","text":"<p>Writes the tile cache to a file, so it does not have to be recomputed</p> <p>The tile cache is normally calculated for each run. However, this can take a long time. By writing it to a file it can be reloaded without the need for recomputation.</p> <p>Limitations: This only works for the exact same model and for a single tile size. If the streaming part of the model changes, or if the tile size is changed, it will no longer work.</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def save_tile_cache_if_needed(self, overwrite=False):\n    \"\"\"\n    Writes the tile cache to a file, so it does not have to be recomputed\n\n    The tile cache is normally calculated for each run.\n    However, this can take a long time. By writing it to a file it can be reloaded without the need\n    for recomputation.\n\n    Limitations:\n    This only works for the exact same model and for a single tile size. If the streaming part of the model\n    changes, or if the tile size is changed, it will no longer work.\n\n    \"\"\"\n    if self.tile_cache_fname is None:\n        self.tile_cache_fname = \"tile_cache_\" + \"1_3_\" + str(self.tile_size) + \"_\" + str(self.tile_size)\n    write_path = Path(self.tile_cache_dir) / Path(self.tile_cache_fname)\n\n    if Path(self.tile_cache_dir).exists():\n        if write_path.exists() and not overwrite:\n            print(\"previous tile cache found and overwrite is false, not saving\")\n\n        elif self.global_rank == 0:\n            print(f\"writing streaming cache file to {str(write_path)}\")\n            torch.save(self.stream_network.get_tile_cache(), str(write_path))\n\n        else:\n            print(\"\")\n    else:\n        raise NotADirectoryError(f\"Did not find {self.tile_cache_dir} or does not exist\")\n</code></pre>"},{"location":"tutorials/classification/","title":"Basic image classification","text":"<p>This tutorial briefly introduces the <code>model</code> repository to easily prototype streaming-capable models right off the bat. The workflow aims to follow the core design principles of the <code>lightning</code> framework, and will not deviate much from it.</p>"},{"location":"tutorials/classification/#training-a-resnet-architecture-using-streaming","title":"Training a ResNet architecture using streaming","text":"<p>For this example, we will use a ResNet-18 model architecture and train it on the Camelyon16 dataset. We assume that the reader is familiar with the regular workflow of a pytorch-lightning model. If this is not the case, please consult the lightning documentation for further information.</p>"},{"location":"tutorials/classification/#importing-the-relevant-packages","title":"Importing the relevant packages","text":"<pre><code>import torch\nimport pyvips\nimport pandas as pd\nimport albumentationsxl as A\n\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom lightstream.models.resnet.resnet import StreamingResNet\nfrom sklearn.model_selection import train_test_split\nfrom lightning.pytorch import Trainer\n</code></pre> <p>We start by importing the relevant packages. The model repository inside the lightstream package comes with a streaming-capable version of the ResNet architectures. We also recommend installing the albumentationsxl package. This package is an albumentations-like augmentation package with a pyvips backend to facilitate loading and transforming large images.</p> <p>Note that for this example, we will be training on a relatively low resolution, since training on high resolutions take a long time to finish.</p>"},{"location":"tutorials/classification/#some-general-settings","title":"Some general settings","text":"<p>We define the paths to the data directory where the images reside, as well as open the label csv file with the image names and labels. For this example we are solving a binary classification problem: tumor versus no tumor within a slide. The dataset is split into a training, validation, and test set, and we define the desired transformations using the albumentationsxl package. For this example, we will only use flips, and then cast them to float tensors.</p> <pre><code>ROOT_DIR = Path(\"/data/pathology/archives/breast/camelyon/CAMELYON16\")\nlabel_df = pd.read_csv(str(ROOT_DIR / Path(\"evaluation/reference.csv\")))\nimage_dir = ROOT_DIR / Path(\"images\")\n\nlabel_df[\"label\"] = label_df[\"class\"].apply(lambda x: 0 if x ==\"negative\" else 1)\n#%%\ntest_df = label_df[label_df[\"image\"].str.startswith(\"test\")]\ntrain_df = label_df[label_df[\"image\"].str.startswith(\"normal\") | label_df[\"image\"].str.startswith(\"tumor\")]\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42,stratify=train_df[\"label\"])\n#%%\n# Normalizing with imagenet statistics is done during streaming in scnn.py, so we don't do that here\nimage_size=4096\ntrain_transforms  = A.Compose([A.CropOrPad(image_size, image_size), A.Flip(p=0.5), A.ToDtype(\"float\", scale=True), A.ToTensor()])\ntest_transforms = A.Compose([A.CropOrPad(image_size, image_size), A.ToDtype(\"float\", scale=True), A.ToTensor()])\n</code></pre>"},{"location":"tutorials/classification/#defining-the-dataloader-and-model","title":"Defining the dataloader and model","text":"<p>We first define the dataset class for the Camelyon16 dataset. We only need the csv file with image names and the label, as well as the path to the image directory. The albumentationsxl package also makes it a straightforward process to open and additionally augment images. Finally, we construct the dataloaders with 1 worker. Typically, you would want to set this number higher, but remember that we are working with large images, so setting <code>num_workers</code> to a high value can lead to out of memory errors.</p> <pre><code>class CamelyonDataset(Dataset):\n    def __init__(self, image_dir: list, df: pd.DataFrame, transform: A.Compose| None=None):\n        self.image_dir = image_dir\n        self.df = df\n        self.transforms = transform\n        self.df[\"image_path\"] = self.df[\"image\"].apply(lambda x: image_dir / Path(x).with_suffix(\".tif\"))\n\n        self.images = self.df[\"image_path\"].tolist()\n        self.labels = self.df[\"label\"].tolist()\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, item):\n        try:\n            image = pyvips.Image.new_from_file(self.images[item], level=5)[0:3]\n        except Exception as e:\n            image = pyvips.Image.new_from_file(self.images[item], page=5)[0:3]\n\n        label = self.labels[item]\n        if self.transforms is not None:\n            image = self.transforms(image=image)[\"image\"]\n        return image, label\n\n\ntrain_loader = DataLoader(dataset=CamelyonDataset(image_dir=image_dir, df=train_df, transform=train_transforms), num_workers=1)\nvalid_loader = DataLoader(dataset=CamelyonDataset(image_dir=image_dir, df=val_df, transform=test_transforms), num_workers=1)\ntest_loader = DataLoader(dataset=CamelyonDataset(image_dir=image_dir, df=test_df, transform=test_transforms), num_workers=1)\n</code></pre>"},{"location":"tutorials/classification/#defining-the-streaming-model","title":"Defining the streaming model","text":"<p>We will now define a ResNet model that is streamable. The code can be found below. We initialize the model with the following values:</p> <ul> <li><code>model_name</code>: A string defining the specific ResNet architecture, in this case ResNet-18</li> <li><code>tile_size</code>: 2880x2880: Streaming processes large images sequentially in tiles (or patches), which are stored in a later layer of the model, and then reconstructed into a whole feature map. Higher tile sizes will typically require more VRAM, but will speed up computations.</li> <li><code>loss_fn</code> : <code>torch.nn.functional.cross_entropy</code>. The loss function for the network. </li> <li><code>num_classes</code>: 2. The number of classes to predict. The default is 1000 classes (ImageNet) default. If a different number is specified, then the <code>fc</code> layer of the ResNet model is re-initialized with random weight and <code>num_classes</code> output neurons.</li> <li><code>train_streaming_layers</code>: False. Whether to train the streaming layers of the ResNet18, we set it to false here (see the reason why below)</li> </ul> <p></p><pre><code>model = StreamingResNet(model_name=\"resnet18\", tile_size=2880, num_classes=2, train_streaming_layers=False, loss_fn=torch.nn.CrossEntropyLoss())\n</code></pre> With this function, we defined an ImageNet pre-trained ResNet-18 model where all of the ResNet-18 layers are now streamed instead of trained directly. The final layers of the model, specifically the global pooling layers and fully connected layers, are not streamed/streamable. Notice that we set <code>train_streaming_layers=False</code> here.  <p>This means that only the parameters of the head classifier will be trainable. We do this for two reasons. First, the classifier head is randomly initialized, and it is recommended to first finetune the randomized parameters in the model, keeping the ImageNet weights frozen, to stabilize training and prevent divergence. Secondly, training all the parameters in the model will add additional computational time during the backward pass. In this example, we do not care about achieving SOTA, but rather show how to train a streaming model.</p> <p>When the streaming function is executed, you will usually see an output similar to what is shown below. These are tile statistics that are calculated under the hood and it shows how many pixels are invalid due to padding within the layers. This is shown for each consecutive convolution and pooling layer within the model. If at any point the sum of the bottom+top, or left+right statistics exceed the tile size for any layer, then the model will not be properly defined, and you will need to increase the tile size.</p> <pre><code>metrics None\nNo tile cache found, calculating it now\n\nConverting modules to nn.Identity()\nExecuting pre-streaming initialization callbacks (if any):\n\nInitializing streaming model\nConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) \n Lost(top:2.0, left:2.0, bottom:1.0, right:1.0)\nMaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) \n Lost(top:2.0, left:2.0, bottom:1.0, right:1.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:3.0, left:3.0, bottom:2.0, right:2.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:5.0, left:5.0, bottom:4.0, right:4.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:6.0, left:6.0, bottom:5.0, right:5.0)\nConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:5.0, left:5.0, bottom:4.0, right:4.0)\nConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:3.0, left:3.0, bottom:2.0, right:2.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:6.0, left:6.0, bottom:5.0, right:5.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:7.0, left:7.0, bottom:6.0, right:6.0)\nConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:5.0, left:5.0, bottom:4.0, right:4.0)\nConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:6.0, left:6.0, bottom:5.0, right:5.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:7.0, left:7.0, bottom:6.0, right:6.0)\nConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:5.0, left:5.0, bottom:4.0, right:4.0)\nConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:4.0, left:4.0, bottom:3.0, right:3.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:6.0, left:6.0, bottom:5.0, right:5.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:7.0, left:7.0, bottom:6.0, right:6.0)\n\n Output lost Lost(top:7.0, left:7.0, bottom:6.0, right:6.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:7.0, left:7.0, bottom:6.0, right:6.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:8.0, left:8.0, bottom:7.0, right:7.0)\nConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:9.0, left:9.0, bottom:8.0, right:8.0)\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:9.0, left:9.0, bottom:8.0, right:8.0)\nConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:10.0, left:10.0, bottom:9.0, right:9.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:22.0, left:22.0, bottom:21.0, right:21.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:23.0, left:23.0, bottom:22.0, right:22.0)\nConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:24.0, left:24.0, bottom:23.0, right:23.0)\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:24.0, left:24.0, bottom:23.0, right:23.0)\nConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:25.0, left:25.0, bottom:24.0, right:24.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:52.0, left:52.0, bottom:51.0, right:51.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:52.0, left:52.0, bottom:51.0, right:51.0)\nConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) \n Lost(top:56.0, left:56.0, bottom:55.0, right:55.0)\nConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:56.0, left:56.0, bottom:55.0, right:55.0)\nConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) \n Lost(top:55.0, left:55.0, bottom:54.0, right:54.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:112.0, left:112.0, bottom:111.0, right:111.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:113.0, left:113.0, bottom:112.0, right:112.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:120.0, left:120.0, bottom:119.0, right:119.0)\nConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) \n Lost(top:119.0, left:119.0, bottom:118.0, right:118.0)\ntesting shape gradient fix\nMaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) \n Lost(top:120.0, left:120.0, bottom:119.0, right:119.0)\nConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) \n Lost(top:241.0, left:241.0, bottom:239.0, right:239.0)\n\n Input gradient lost Lost(top:490.0, left:490.0, bottom:487.0, right:487.0)\nRestoring model weights\nExecuting post-streaming initialization callbacks (if any):\n\nwriting streaming cache file to /tmp/pycharm_project_341/notebooks/tile_cache_1_3_2880_2880\nWARNING: Streaming network will not be trained\n</code></pre>"},{"location":"tutorials/classification/#training-the-model","title":"Training the model","text":"<p>Finally, we define a regular pytorch lightning trainer and we can fit the model as usual.</p> <pre><code>model = StreamingResNet(model_name=\"resnet18\", tile_size=2880, num_classes=2, train_streaming_layers=False, loss_fn=torch.nn.CrossEntropyLoss())\n#%%\ntrainer = Trainer(\n        default_root_dir=\"./\",\n        accelerator=\"gpu\",\n        max_epochs=15,\n        devices=1,\n        precision=\"16-mixed\",\n        strategy=\"auto\",\n    )\n\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n</code></pre>"},{"location":"tutorials/custom_models/","title":"Custom models","text":"<p>Custom models can be created in one of three ways:</p> <ul> <li>Using the <code>StreamingModule</code> (recommended)</li> <li>Using the <code>ImageNetClassifier</code> (a subclass of <code>StreamingModule</code>)</li> <li>Creating your own class (not recommended)</li> </ul> <p>The <code>StreamingModule</code> and <code>ImageNetClassifier</code> classes are both regular <code>LightningModule</code> objects and should be treated as such. Both classes have several helper functions and a custom initialization that create the streaming model instance. Secondly,  the helper functions make sure that several settings, such as freezing normalization layers and setting them to <code>eval()</code> mode, both during training and inference. This is necessary since streaming does not work with layers that are not locally defined, but rather need the entire input image.</p> <p>Warning</p> <p>Please consult this documentation thoroughly before creating your own module. Do not carelessly override the following callbacks/hooks: on_train_epoch_start, on_train_start, on_validation_start, on_test_start</p>"},{"location":"tutorials/custom_models/#model-prerequisites","title":"Model prerequisites","text":"<p>Before implementing a streaming version of your model, make sure that the following conditions hold:</p> <ul> <li>Your model is at least partly a CNN</li> <li>Within the CNN, fully connected layers or global pooling layers are not used. This also means it should not have Squeeze and Excite (SE) blocks, since these are global operations, rather than local.<ul> <li>A small exception to this are normalization layers. If your model contains any, they must be set to <code>eval()</code> both during inference and training. Since most normalization require the entire input to correctly calculate means and standard deviations, they will theoretically not work with streaming during training. During inference, the means and standard deviations can be applied tile-wise.</li> </ul> </li> </ul>"},{"location":"tutorials/custom_models/#splitting-and-creating-the-models","title":"Splitting and creating the model(s)","text":"<p>Usually a CNN contains a part that can be streamed, and a part that cannot be used with streaming. In the case of e.g. a ResNet architecture, the <code>fc</code> layers of the model contains global pooling and fully connected layers, and thus are not fit for streaming.  That's why models are usually split. The correct model can then be made streamable using the <code>StreamingModule</code>\"</p> <pre><code>def split_resnet(net, **kwargs):\n    \"\"\" Split resnet architectures into backbone and fc modules\n\n    The stream_net will contain the CNN backbone that is capable for streaming.\n    The fc model is not streamable and will be a separate module\n    If num_classes are specified as a kwarg, then a new fc model will be created with the desired classes\n\n    Parameters\n    ----------\n    net: torch model\n        A ResNet model in the format provided by torchvision\n    kwargs\n\n    Returns\n    -------\n    stream_net : torch.nn.Sequential\n        The CNN backbone of the ResNet\n    head : torch.nn.Sequential\n        The head of the model, defaults to the fc model provided by torchvision.\n\n    \"\"\"\n\n\n    num_classes = kwargs.get(\"num_classes\", 1000)\n    stream_net = nn.Sequential(\n        net.conv1, net.bn1, net.relu, net.maxpool, net.layer1, net.layer2, net.layer3, net.layer4\n    )\n\n    # 1000 classes is the default from ImageNet classification\n    if num_classes != 1000:\n        net.fc = torch.nn.Linear(512, num_classes)\n        torch.nn.init.xavier_normal_(net.fc.weight)\n        net.fc.bias.data.fill_(0)  # type:ignore\n\n    head = nn.Sequential(net.avgpool, nn.Flatten(), net.fc)\n\n    return stream_net, head\n</code></pre> <p>After defining the model split, the <code>StreamingModule</code> or <code>BaseModule</code> can be used to create a model consisting of a streamable CNN backbone and optional head networks. Both the  <code>StreamingModule</code> and <code>BaseModule</code> inherit from <code>lightning.LightningModule</code>, meaning all the regular pytorch lightning functions and workflows become available here well. The streamable ResNet is defined below:</p> <p></p><pre><code>class StreamingResNet(StreamingModule):\n    model_choices = {\"resnet18\": resnet18, \"resnet34\": resnet34, \"resnet50\": resnet50}\n\n    def __init__(\n        self,\n        model_name: str,\n        tile_size: int,\n        loss_fn: torch.nn.functional,\n        train_streaming_layers: bool = True,\n        metrics: MetricCollection | None = None,\n        **kwargs\n    ):\n        assert model_name in list(StreamingResNet.model_choices.keys())\n        network = StreamingResNet.model_choices[model_name](weights=\"DEFAULT\")\n        stream_network, head = split_resnet(network, num_classes=kwargs.pop(\"num_classes\", 1000))\n\n        self._get_streaming_options(**kwargs)\n        print(\"metrics\", metrics)\n        super().__init__(\n            stream_network,\n            head,\n            tile_size,\n            loss_fn,\n            train_streaming_layers=train_streaming_layers,\n            metrics=metrics,\n            **self.streaming_options,\n        )\n\n    def _get_streaming_options(self, **kwargs):\n        \"\"\"Set streaming defaults, but overwrite them with values of kwargs if present.\"\"\"\n\n        # We need to add torch.nn.Batchnorm to the keep modules, because of some in-place ops error if we don't\n        # https://discuss.pytorch.org/t/register-full-backward-hook-for-residual-connection/146850\n        streaming_options = {\n            \"verbose\": True,\n            \"copy_to_gpu\": False,\n            \"statistics_on_cpu\": True,\n            \"normalize_on_gpu\": True,\n            \"mean\": [0.485, 0.456, 0.406],\n            \"std\": [0.229, 0.224, 0.225],\n            \"add_keep_modules\": [torch.nn.BatchNorm2d],\n        }\n        self.streaming_options = {**streaming_options, **kwargs}\n</code></pre> The actual streaming module can be configured with varying settings. These can be passed as a kwarg dictionary within the <code>super().__init__()</code> call of the parent class. Under the hood, this dictionary with options is passed to a constructor which takes care of properly building the streaming module. A more detailed explanation about the constructor can be found below."},{"location":"tutorials/custom_models/#custom-forwardbackward-logic","title":"Custom forward/backward logic","text":"<p>Since we inherit directory from the lightning modules, the routine for forward and backpropagation remains mostly similar to that of pytorch lightning. However, there are a few tweaks and tricks that must be taken into account:</p> <ul> <li>Forward pass: Can be run as a usual, but we recommend making the input image accessible via self: i.e. <code>self.image = image</code></li> </ul> <pre><code>def forward_head(self, x):\n    return self.head(x)\n\ndef forward(self, x):\n    fmap = self.forward_streaming(x)\n    out = self.forward_head(fmap)\n    return out\n</code></pre> <ul> <li>Backward pass/training step: The StreamingCNN object that defines the streaming network requires the gradient and input image as input. To actually backpropagate the input, you can use the <code>backward_streaming</code> function which requires the input image and the gradient of the head of the model.</li> </ul> <pre><code>def training_step(self, batch: Any, batch_idx: int, *args: Any, **kwargs: Any) -&gt; tuple[Any, Any, Any]:\n    image, target = batch\n\n    # This is needed later in the backward function!\n    self.image = image\n\n    self.str_output = self.forward_streaming(image)\n\n    if self.use_streaming:\n        self.str_output.requires_grad = self.training\n\n    out = self.forward_head(self.str_output)\n    loss = self.loss_fn(out, target)\n\n    self.log_dict({\"entropy loss\": loss.detach()}, prog_bar=True)\n    return loss\n\ndef backward(self, loss):\n    loss.backward()\n    if self.train_streaming_layers and self.use_streaming:\n        self.backward_streaming(self.image, self.str_output.grad)\n    del self.str_output\n</code></pre> <ul> <li>Hooks: Several hooks in pytorch lightning are used to set the normalization layers to <code>eval()</code> and set the inputs/models to the right device.<ul> <li>on_training_start: Allocates the input and models to the correct device at training time.</li> <li>on_validation_start: Allocates the input and models to the correct device at validation time.</li> <li>on_test_start: Allocates the input and models to the correct device at test time.</li> <li>on_train_epoch_start(self): sets all the normalization layers to eval() during training</li> </ul> </li> </ul> <p>Warning: do not override these hooks with your own code. If you need these hooks for any reason, then call the parent method first using e.g.  <code>super().on_training_start</code></p>"},{"location":"tutorials/custom_models/#streaming-using-the-constructor","title":"Streaming using the constructor","text":"<p>Under the hood of the <code>StreamingModule</code> class, we have an additional <code>Constructor</code> class that actually builds and defines the streaming module. The following arguments can be passed to it: At the very least, a torch model and a tile size must be provided. </p><pre><code>model: torch.nn.modules,\ntile_size: int,\nverbose: bool = False,\ndeterministic: bool = False,\nsaliency: bool = False,\ncopy_to_gpu: bool = False,\nstatistics_on_cpu: bool = False,\nnormalize_on_gpu: bool = False,\nmean: list[float, float, float] | None = None,\nstd: list[float, float, float] | None = None,\ntile_cache: dict | None = None,\nadd_keep_modules: list[torch.nn.modules] | None = None,\nbefore_streaming_init_callbacks: list[Callable[[torch.nn.modules], None], ...] | None = None,\nafter_streaming_init_callbacks: list[Callable[[torch.nn.modules], None], ...] | None = None,\n</code></pre>"},{"location":"tutorials/custom_models/#constructor-default-behaviour","title":"Constructor default behaviour","text":"<p>By default, the constructor will perform the following steps:  1. All layers except convolution, local max pooling, and local average pooling layers are set to <code>nn.Identity</code>  2. <code>before_streaming_init_callbacks</code> are executed. These are user-specified, and by default, no callbacks are executed.  3. The streaming module is constructed from the convolution/local pooling layers. Within this step, the model's weights are altered to calculate tile statistics.  4. The streaming module's <code>nn.Identity</code> layers from step 1 are restored back to their old layers, and the correct model weights are reloaded  5. <code>after_streaming_init_callbacks</code> are executed. These are user-specified, and by default, no callbacks are executed.  6. The streaming module is returned</p> <p>The before and after streaming initialization callbacks are added for flexibility, since we cannot take all possible variations for model creation into account.  An example of where these callbacks come in handy is given in the code for the streaming convnext model. Within this model, we need to additionally turn off the stochastic depth operation, as well as the layer scale, which are not normal layers. For a more detailed example, we invite the reader to look at the code for either the Resnet or Convnext models within the repository.</p>"},{"location":"tutorials/dataloading/","title":"Image processing using pyvips and albumentationsxl","text":"<p>For this tutorial, we will be using the pyvips backend to load and manipulate images. Pyvips was specifically built with  large images in mind. It builds data pipelines for each image, rather than directly loading it into memory. As a result, it can keep a low memory footprint during execution, whilst still being fast. </p> <p>Secondly, we will be using the albumentationsxl package. This is virtually the same package as albumentations, but using the pyvips backend. It features a wide range of image augmentations capable of transforming large images specifically.</p> <p>Within this tutorial we will be using the Imagenette dataset to serve as an example. Notice that these images are small enough to fit in memory. The example below only serves as a demonstration of how the data augmentation works with a pyvips backend, and should also work with large images.</p>"},{"location":"tutorials/dataloading/#an-example-using-imagenette","title":"An example using Imagenette","text":""},{"location":"tutorials/dataloading/#downloading-and-extracting-imagenette-data","title":"Downloading and extracting ImageNette data","text":"<p>We start by downloading and extracting the ImageNette dataset into the data folder, which is stored in the current working directory. The data is then extracted using the <code>extract_all_files</code> function</p> <pre><code>import os\nimport pyvips\nimport albumentationsxl as A\nfrom pathlib import Path\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import Dataset\nimport tarfile\n\n\ndef extract_all_files(tar_file_path, extract_to):\n    with tarfile.open(tar_file_path, \"r\") as tar:\n        tar.extractall(extract_to)\n\n\ndef download_and_extract():\n    # Download dataset and extract in a data/ directory\n    if not os.path.isfile(\"data/imagenette2-320.tgz\"):\n        print(\"Downloading dataset\")\n        download_url(\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\", os.getcwd() + \"/data\")\n        print(\"Extracting dataset\")\n        extract_all_files(os.getcwd() + \"/data/imagenette2-320.tgz\", os.getcwd() + \"/data\")\n</code></pre>"},{"location":"tutorials/dataloading/#define-the-dataloader","title":"Define the dataloader","text":"<p>Defining a pytorch dataset with pyvips is straightforward and does not require much tweaking from normal pipelines. In a nutshell, the following changes are made from a normal pipeline:</p> <ul> <li>Instead of opening image files using PIL, torch, cv2, etc... we are opening the file with a pyvips backend <code>pyvips.Image.new_from_file()</code></li> <li>The albumentations backend is replaced with the albumentationsxl package. </li> </ul> <pre><code>class ImagenetteDataset(Dataset):\n    def __init__(self, patch_size=320, validation=False):\n        self.folder = Path(\"data/imagenette2-320/train\") if not validation else Path(\"data/imagenette2-320/val\")\n        self.classes = [ \"n01440764\", \"n02102040\", \"n02979186\", \"n03000684\", \"n03028079\", \"n03394916\", \"n03417042\",\n                         \"n03425413\", \"n03445777\", \"n03888257\"]\n\n        self.images = []\n        for cls in self.classes:\n            cls_images = list(self.folder.glob(cls + \"/*.JPEG\"))\n            self.images.extend(cls_images)\n\n        self.patch_size = patch_size\n        self.validation = validation\n\n        self.transforms = A.Compose(\n            [\n                A.RandomBrightnessContrast(p=1.0),\n                A.Rotate(p=0.8),\n                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                A.ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, index):\n        image_fname = self.images[index]\n        image = pyvips.Image.new_from_file(image_fname)\n        label = image_fname.parent.stem\n        label = self.classes.index(label)\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]\n\n        return image, label\n\n    def __len__(self):\n        return len(self.images)\n</code></pre> <p>Creating custom datasets with large images thus requires little changes and remain flexible enough to use with most of the Pytorch native infrastructure outside of image transformations. Since these are regular Pytorch Dataset objects, it will work with any data, including masks, bounding boxes, or keypoints, as long as they can be converted into Tensors for model training.</p>"},{"location":"tutorials/dataloading/#image-processing-best-practices","title":"Image processing best practices","text":"<p>The dataset used in the example enough was small enough to load into memory using normal image libraries. This was done to keep the tutorial lightweight and easy to execute. In practice larger images will be more of a challenge to optimize correctly. Below we provide several tips to include in your custom pipeline to facilitate fast image processing with lower memory footprints.</p>"},{"location":"tutorials/dataloading/#load-images-as-uint8","title":"load images as uint8","text":"<p>Pyvips images can be loaded or otherwise cast to uint8 (\"uchar\" in pyvips). This will increase the speed of the computations done by pyvips, while also preserving memory. </p>"},{"location":"tutorials/dataloading/#number-of-transformations-and-sequence","title":"Number of transformations and sequence","text":"<p>Image transformations on large images are costly, therefore, make sure not to include any redundant, e.g. mixing <code>Affine</code> with <code>Rotate</code> in the same pipeline, as Rotate is a subset of Affine. Also, some transformations are computationally expensive, such as elastic transforms, so try to avoid using this transformation every time if you are experiencing poor GPU utilization due to cpu bottlenecks.</p> <p>Finally, the <code>Normalize</code> transform will cast the image to a <code>float32</code> format. It is recommended to always put this transformation into the very end of the augmentation pipeline, since float32 operations are costlier than <code>uint8</code>. Failing to do so can introduce bottlenecks in the augmentation pipeline.</p>"},{"location":"tutorials/dataloading/#optional-image-normalization-on-gpu","title":"Optional: image normalization on gpu","text":"<p>within the streaming library, it is possible to normalize the image tile-wise on the gpu instead of in the dataloader. This could improve speed, as well as  lower memory footprints in some scenarios, for example if the image can be stored as an uint8 tensor before converting to e.g. float16. </p>"},{"location":"tutorials/dataloading/#image-memory-requirements","title":"Image memory requirements","text":"<p>We can take a look at the following table on how images grow in size as we increase their size, as well as changing their dtypes. From this table, we can already conclude that it is better to work with uint8 and float16 for training as much a possible. </p> Table 1: All values are in gigabyes (GB). Values generated using random numpy arrays Image size uint8 float16 float32 float64 8192x8192x3 0.2 0.4 0.8 1.6 16384x16384x3 0.8 1.6 3.2 6.4 32768x32768x3 3.2 6.4 12.8 25.6 65536x65536x3 12.8 25.6 51.2 102.4"},{"location":"tutorials/trainer_options/","title":"Trainer settings","text":"<p>The <code>Trainer</code> settings defined in native Pytorch Lightning will</p>"},{"location":"tutorials/trainer_options/#multi-gpu-support","title":"Multi GPU support","text":"<p>Training on multiple gpu's in streaming is handled by Pytorch lightning. However, the native <code>auto</code> option within the <code>Trainer</code> class will not work (we omit the technical details why here). For the <code>strategy</code> argument of the Trainer, we therefore recommend to use the <code>ddp_find_unused_parameters_true</code> instead, which does not conflict with streaming and gradient checkpointing.</p>"},{"location":"tutorials/trainer_options/#gradient-accumulation","title":"Gradient accumulation","text":"<p>Specifying higher batch sizes will not affect normalization layers during training, as they should be on <code>eval()</code> mode. However, gradient accumulation is still possible and can stabilize training under certain circumstances. This can be easily set using the <code>accumulate_grad_batches</code> argument.</p>"},{"location":"tutorials/trainer_options/#precision","title":"Precision","text":"<p>We recommend to train using mixed precision training wherever possible and to let pytorch handle the conversion. This can be set using the <code>16-mixed</code> option.</p>"},{"location":"tutorials/trainer_options/#loggers-and-callbacks","title":"Loggers and callbacks","text":"<p>Callbacks for a variety of training strategies (checkpointing, early stopping, etc) are natively supported by Pytorch Lightning. Please consult their respective documentation on how to do this. The same can be said for standard logging solutions (Tensorboard, Wandb).</p>"},{"location":"tutorials/trainer_options/#example","title":"Example","text":"<pre><code>trainer = pl.Trainer(\n    default_root_dir=\"path_to_save_dir\",\n    accelerator=\"gpu\",\n    max_epochs=100,\n    devices=2,\n    strategy=\"ddp_find_unused_parameters_true\",\n    accumulate_grad_batches=8,\n    precision=\"16-mixed\",\n    logger=wandb_logger,\n)\n</code></pre>"}]}