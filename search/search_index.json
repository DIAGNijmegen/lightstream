{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lightstream","text":"<p>Lightstream is a Pytorch-Lightning library for training CNN-based models with large input data using streaming.  This approach allows you to parse huge (image) inputs through a CNN without running into memory bottlenecks, i.e. getting GPU out of memory (OOM) errors.</p> <p>The underlying algorithm is based on the <code>streaming</code> paper described in [1]. During training/inferencing,  a huge input image that would normally cause GPU OOM is split into tiles and processed sequentially until a pre-defined part of the network.  There, the individual tiles are stitched back together, and the forward/backward is finished normally. Due to gradient  checkpointing, intermediate features are deleted to save memory, and are re-computed tile-wise during backpropagation (see figure below).</p> <p>By doing so, the result is mathematically the same as if the large input was parsed directly through a GPU without memory restrictions.</p> <p>Figure 1: Alt Text</p>"},{"location":"#implemented-in-pytorch-lightning","title":"Implemented in Pytorch-Lightning","text":"<p>The Lightstream package is simple to test and extend as it works with native Pytorch, and also works with Lightning to minimize boilerplate code. Most convolutional neural networks can be easily converted into a streaming equivalent using a simple wrapper in native Pytorch:</p> <pre><code># Resnet18 turned into a streaming-capable network\nfrom torchvision.models import resnet18\nimport torch.nn as nn\n\nstream_layers = nn.Sequential(\n        net.conv1, net.bn1, net.relu, net.maxpool, net.layer1, net.layer2, net.layer3, net.layer4\n    )\n\n\nsCNN = StreamingCNN(stream_layers, tile_shape=(1, 3, 600, 600))\nstr_output = sCNN(image)\n\nfinal_output = final_layers(str_output)\nloss = criterion(final_output, labels)\n\nloss.backward()\nsCNN.backward(image, str_output.grad)\n</code></pre> <p>Warning</p> <p>Not all layers are supported during streaming. To see the caveats, please consult the how-to pages.</p>"},{"location":"#limitations-to-streaming","title":"Limitations to streaming","text":"<p>The streaming algorithm exploits the fact that convolutions are locally defined. This means that the entire input image does not have to be parsed through the network at once, but can be reconstructed piece-wise. There are many layers that do not possess this property, such as batch normalization layers, where the mean and standard deviations computations require the entire image at once.</p>"},{"location":"#layers-that-can-be-used-without-issue","title":"Layers that can be used without issue","text":"<ul> <li>Convolutional layers: can be used without issue, since they are locally defined</li> <li>Pooling layers such as average, max, GEM pooling, as long as they are locally defined, e.g. a 2x2 kernel. Global pooling will not work.</li> <li>Any other layer that is defined locally and not dependent on seeing the entire image at once. </li> </ul>"},{"location":"#layers-that-are-restricted","title":"Layers that are restricted","text":"<ul> <li>All normalization layers: e.g. batch normalization. Most normalization layers require image-level statistics such as means and standard deviations to be computed. As streaming works tile-wise, they will not yield the correct results. Therefore, all normalization layers must be set to <code>eval()</code> during training.</li> <li>Dense layers can only be used to model 1x1 convolutions, i.e. a fully connected layer that works over the channels of an input, rather than the spatial dimensions.</li> </ul>"},{"location":"#references","title":"References","text":"<p>[1]  H. Pinckaers, B. van Ginneken and G. Litjens, \"Streaming convolutional neural networks for end-to-end learning with multi-megapixel images,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence,  doi: 10.1109/TPAMI.2020.3019563</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installing-lightstream","title":"Installing lightstream","text":"<p>Installing the lightstream package can be done via pip: </p><pre><code>pip install lightstream\n</code></pre> <p>Alternative, the project can be cloned using <code>git</code> from the lightstream git repo.  From there, the package can again be installed using <code>pip install .</code> within the directory. Otherwise, you can use the repository as-is, but you will have to add it to your <code>PYTHONPATH</code> if you want to access it from other repositories.</p>"},{"location":"getting_started/installation/#installing-pyvips","title":"Installing (Py)vips","text":"<p>Before pyvips can be installed, libvips must be present on the system, as pyvips is a Python wrapper around the libvips library. Libvips can be installed either using the binaries on the libvips website, or it can be built from source with or without Docker. After libvips is properly installed, pyvips can be installed via pip: </p><pre><code>pip install pyvips\n</code></pre>"},{"location":"getting_started/installation/#installing-torch-related-libraries","title":"Installing torch-related libraries","text":"<p>Installing torch and its related libraries (lightning, torchvision) can be done via pip installs. We recommend looking at their respective websites for further help to install these packages.</p>"},{"location":"getting_started/requirements/","title":"Requirements","text":""},{"location":"getting_started/requirements/#software-requirements","title":"Software Requirements","text":"<p>The lightstream package is supported on Linux distributions. Although it should work on Windows, it is untested and therefore not recommended.</p> <p>The following packages will need to be installed for lightstream to work. Note that albumentationsXL is not strictly required, but features an albumentations-like data augmentations pipeline for processing large images.</p> <ul> <li>Pytorch (2.0.0) or higher</li> <li>Pytorch-lightning (2.0.0) or higher</li> <li>torchvision (0.15 or higher)</li> <li>Pyvips</li> <li>Requires libvips to be installed and configured.</li> <li>albumentationsXL (optional, but recommended)</li> </ul>"},{"location":"getting_started/requirements/#hardware-requirements","title":"Hardware requirements","text":"<ul> <li> <p>GPU: Considering lightstream is computationally heavy, it is highly recommend to use a GPU with at least 10GB VRAM instead of CPU backends.</p> </li> <li> <p>RAM: Additionally, large amounts of RAM are highly recommended (at least 32 GB). Although pyvips will keep memory footprints low during execution, they will have to be stored in RAM before sending it over to the GPU. Since we are dealing with large images, this can become quite costly. To give an idea of the RAM cost, we have included a reference table of the approximate memory footprint of several image sizes with varying dtypes. Given the values in table 1, it is recommended to work with np.uint8 as much as possible, and only use float32 sparingly.  Float64 images are usually unnecessary and should be avoided entirely.</p> </li> </ul> Table 1: All values are in gigabyes (GB) Image size uint8 float16 float32 float64 8192x8192x3 0.2 0.4 0.8 1.6 16384x16384x3 0.8 1.6 3.2 6.4 32768x32768x3 3.2 6.4 12.8 25.6 65536x65536x3 12.8 25.6 51.2 102.4"},{"location":"models/resnet/","title":"Resnet","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>lightstream\\models\\resnet\\resnet.py</code> <pre><code>class StreamingResNet(BaseModel):\n    model_choices = {\"resnet18\": resnet18, \"resnet34\": resnet34, \"resnet50\": resnet50}\n\n    def __init__(\n        self,\n        model_name: str,\n        tile_size: int,\n        loss_fn: torch.nn.functional,\n        train_streaming_layers: bool = True,\n        use_streaming: bool = True,\n        *args,\n        **kwargs\n    ):\n        assert model_name in list(StreamingResNet.model_choices.keys())\n        network = StreamingResNet.model_choices[model_name](weights=\"IMAGENET1K_V1\")\n        stream_net, head = split_resnet(network, num_classes=kwargs.get(\"num_classes\", 1000))\n        super().__init__(\n            stream_net,\n            head,\n            tile_size,\n            loss_fn,\n            train_streaming_layers=train_streaming_layers,\n            use_streaming=use_streaming,\n            *args,\n            **kwargs\n        )\n</code></pre>"},{"location":"modules/basemodule/","title":"BaseModule","text":"<p>             Bases: <code>StreamingModule</code></p> Source code in <code>lightstream\\modules\\base.py</code> <pre><code>class BaseModel(StreamingModule):\n    def __init__(\n        self,\n        stream_net: torch.nn.modules.container.Sequential,\n        head: torch.nn.modules.container.Sequential,\n        tile_size: int,\n        loss_fn: torch.nn.modules.loss,\n        train_streaming_layers=True,\n        use_streaming=True,\n        *args,\n        **kwargs\n    ):\n        super().__init__(stream_net, tile_size, use_streaming, train_streaming_layers, *args, **kwargs)\n        self.head = head\n        self.loss_fn = loss_fn\n        self.train_streaming_layers = train_streaming_layers\n        self.params = self.extend_trainable_params()\n\n    def on_train_epoch_start(self) -&gt; None:\n        super().on_train_epoch_start()\n\n    def forward_head(self, x):\n        return self.head(x)\n\n    def forward(self, x):\n        fmap = self.forward_streaming(x)\n        out = self.forward_head(fmap)\n        return out\n\n    def training_step(self, batch: Any, batch_idx: int, *args: Any, **kwargs: Any) -&gt; tuple[Any, Any, Any]:\n        image, target = batch\n        self.image = image\n\n        self.str_output = self.forward_streaming(image)\n\n        if self.use_streaming:\n            self.str_output.requires_grad = self.training\n\n        out = self.forward_head(self.str_output)\n        loss = self.loss_fn(out, target)\n\n        self.log_dict({\"entropy loss\": loss.detach()}, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.params, lr=1e-3)\n        return opt\n\n    def extend_trainable_params(self):\n        if self.params:\n            return self.params + list(self.head.parameters())\n        return list(self.head.parameters())\n\n    def backward(self, loss):\n        loss.backward()\n        # del loss\n        # Don't call this&gt;? https://pytorch-lightning.readthedocs.io/en/1.5.10/guides/speed.html#things-to-avoid\n        torch.cuda.empty_cache()\n        if self.train_streaming_layers and self.use_streaming:\n            self.backward_streaming(self.image, self.str_output.grad)\n        del self.str_output\n</code></pre>"},{"location":"modules/streamingmodule/","title":"StreamingModule","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>class StreamingModule(L.LightningModule):\n    def __init__(self, stream_network, tile_size, use_streaming=True, train_streaming_layers=True, *args, **kwargs):\n        super().__init__()\n        self.use_streaming = use_streaming\n        self.train_streaming_layers = train_streaming_layers\n        self._stream_module = stream_network\n        self.params = self.get_trainable_params()\n\n        # StreamingCNN options\n        self.tile_size = tile_size\n        self.deterministic = kwargs.get(\"deterministic\", True)\n        self.saliency = kwargs.get(\"saliency\", False)\n        self.copy_to_gpu = kwargs.get(\"copy_to_gpu\", False)\n        self.verbose = kwargs.get(\"verbose\", True)\n        self.mean = kwargs.get(\"mean\", [0.485, 0.456, 0.406])\n        self.std = kwargs.get(\"std\", [0.229, 0.224, 0.225])\n        self.statistics_on_cpu = kwargs.get(\"statistics_on_cpu\", True)\n        self.normalize_on_gpu = kwargs.get(\"normalize_on_gpu\", False)\n\n        if not self.statistics_on_cpu:\n            # Move to cuda manually if statistics are computed on gpu\n            device = torch.device(\"cuda\")\n            stream_network.to(device)\n\n        self.stream_network = StreamingCNN(\n            stream_network,\n            tile_shape=(1, 3, tile_size, tile_size),\n            deterministic=self.deterministic,\n            saliency=self.saliency,\n            copy_to_gpu=self.copy_to_gpu,\n            verbose=self.verbose,\n            statistics_on_cpu=self.statistics_on_cpu,\n            normalize_on_gpu=self.normalize_on_gpu,\n            mean=self.mean,\n            std=self.std,\n            state_dict=kwargs.get(\"state_dict\", None),\n        )\n\n        if not self.use_streaming:\n            self.disable_streaming()\n\n    def freeze_streaming_normalization_layers(self):\n        \"\"\"Do not use normalization layers within lightstream, only local ops are allowed\"\"\"\n        freeze_layers = [l for l in self.stream_network.stream_module.modules() if isinstance(l, torch.nn.BatchNorm2d)]\n\n        for mod in freeze_layers:\n            mod.eval()\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\" on_train_epoch_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        self.freeze_streaming_normalization_layers()\n\n    def on_validation_start(self):\n        \"\"\" on_validation_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        # Update streaming to put all the inputs/tensors on the right device\n        self.stream_network.device = self.device\n        self.stream_network.mean = self.mean\n        self.stream_network.std = self.std\n        self.stream_network.dtype = self.dtype\n\n    def on_train_start(self):\n        \"\"\" on_train_start hook\n\n        Do not override this method. Instead, call the parent class using super().on_train_start if you want\n        to add this hook into your pipelines\n\n        \"\"\"\n        # Update streaming to put all the inputs/tensors on the right device\n        self.stream_network.device = self.device\n        self.stream_network.mean = self.mean\n        self.stream_network.std = self.std\n        self.stream_network.dtype = self.dtype\n\n    def disable_streaming(self):\n        \"\"\"Disable streaming hooks and replace streamingconv2d  with conv2d modules\n\n        This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient\n        checkpointing will be turned off.\n        \"\"\"\n        self.stream_network.disable()\n        self.use_streaming = False\n\n    def enable_streaming(self):\n        \"\"\"Enable streaming hooks and use streamingconv2d modules\"\"\"\n        self.stream_network.enable()\n        self.use_streaming = True\n\n    def forward_streaming(self, x):\n        \"\"\"\n\n        Parameters\n        ----------\n        x : torch.Tensor\n        The input tensor in [1,C,H,W] format\n\n        Returns\n        -------\n        out: torch.Tensor\n        The output of the streaming model\n\n        \"\"\"\n        out = self.stream_network(x) if self.use_streaming else self.stream_network.stream_module(x)\n        return out\n\n    def backward_streaming(self, image, gradient):\n        \"\"\" Perform the backward pass using the streaming network\n\n        Backward only if streaming is turned on.\n        This method is primarily a convenience function\n\n        Parameters\n        ----------\n        image: torch.Tensor\n            The input image in [1,C,H,W] format\n        gradient: torch.Tensor\n            The gradient of the next layer in the model to continue backpropagation with\n\n        Returns\n        -------\n\n        \"\"\"\n\n        if self.use_streaming and self.train_streaming_layers:\n            self.stream_network.backward(image, gradient)\n\n    def configure_tile_delta(self):\n        \"\"\"\n        Helper function that returns the tile stride during streaming.\n\n        Streaming assumes that the input image is perfectly divisible with the network output stride or the\n        tile stride. This function will return the tile stride, which can then be used within data processing pipelines\n        to pad/crop images to a multiple of the tile stride.\n\n        Examples:\n\n        Returns\n        -------\n        tile_delta: numpy.ndarray\n            the tile stride.\n\n\n        \"\"\"\n        delta = self.tile_size - (\n            self.stream_network.tile_gradient_lost.left + self.stream_network.tile_gradient_lost.right\n        )\n        delta = delta // self.stream_network.output_stride[-1]\n        delta *= self.stream_network.output_stride[-1]\n        return delta.detach().cpu().numpy()\n\n    def get_trainable_params(self):\n        \"\"\" Get trainable parameters for the entire model\n\n        If self.streaming_layers is True, then the parameters of the streaming network will be trained.\n        Otherwise, the parameters will be left untrained (no gradients will be collected)\n\n        \"\"\"\n        if self.train_streaming_layers:\n            params = list(self._stream_module.parameters())\n            return params\n        else:\n            print(\"WARNING: Streaming network will not be trained\")\n            for param in self._stream_module.parameters():\n                param.requires_grad = False\n\n    def _remove_streaming_network(self):\n        \"\"\"Converts the streaming network into a non-streaming network\n\n        The former streaming encoder can be addressed as self.stream_network\n        This function is currently untested and breaks the class, since there is no way to rebuild the streaming network\n        other than calling a new class directly.\n\n        \"\"\"\n\n        # Convert streamingConv2D into regular Conv2D and turn off streaming hooks\n        self.disable_streaming()\n        self.use_streaming = False\n        temp = self.stream_network.stream_module\n\n        # torch modules cannot be overridden normally, so delete and reassign\n        del self.stream_network\n        self.stream_network = temp\n\n    def _build_streaming_network(self, **kwargs):\n        \"\"\"\n            (re)-build the streaming network\n        \"\"\"\n\n        stream_network = self.stream_network\n        del self.stream_network\n\n        self.stream_network = StreamingCNN(\n            stream_network,\n            tile_shape=(1, 3, self.tile_size, self.tile_size),\n            deterministic=kwargs.get(\"deterministic\", True),\n            saliency=kwargs.get(\"saliency\", False),\n            gather_gradients=kwargs.get(\"gather_gradients\", False),\n            copy_to_gpu=kwargs.get(\"copy_to_gpu\", True),\n            verbose=kwargs.get(\"verbose\", True),\n            statistics_on_cpu=kwargs.get(\"statistics_on_cpu\", False),\n            normalize_on_gpu=kwargs.get(\"normalize_on_gpu\", False),\n            mean=self.mean,\n            std=self.std,\n            state_dict=kwargs.get(\"state_dict\", None),\n        )\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.backward_streaming","title":"<code>backward_streaming(image, gradient)</code>","text":"<p>Perform the backward pass using the streaming network</p> <p>Backward only if streaming is turned on. This method is primarily a convenience function</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>The input image in [1,C,H,W] format</p> required <code>gradient</code> <p>The gradient of the next layer in the model to continue backpropagation with</p> required Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def backward_streaming(self, image, gradient):\n    \"\"\" Perform the backward pass using the streaming network\n\n    Backward only if streaming is turned on.\n    This method is primarily a convenience function\n\n    Parameters\n    ----------\n    image: torch.Tensor\n        The input image in [1,C,H,W] format\n    gradient: torch.Tensor\n        The gradient of the next layer in the model to continue backpropagation with\n\n    Returns\n    -------\n\n    \"\"\"\n\n    if self.use_streaming and self.train_streaming_layers:\n        self.stream_network.backward(image, gradient)\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.configure_tile_delta","title":"<code>configure_tile_delta()</code>","text":"<p>Helper function that returns the tile stride during streaming.</p> <p>Streaming assumes that the input image is perfectly divisible with the network output stride or the tile stride. This function will return the tile stride, which can then be used within data processing pipelines to pad/crop images to a multiple of the tile stride.</p> <p>Examples:</p> <p>Returns:</p> Name Type Description <code>tile_delta</code> <code>ndarray</code> <p>the tile stride.</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def configure_tile_delta(self):\n    \"\"\"\n    Helper function that returns the tile stride during streaming.\n\n    Streaming assumes that the input image is perfectly divisible with the network output stride or the\n    tile stride. This function will return the tile stride, which can then be used within data processing pipelines\n    to pad/crop images to a multiple of the tile stride.\n\n    Examples:\n\n    Returns\n    -------\n    tile_delta: numpy.ndarray\n        the tile stride.\n\n\n    \"\"\"\n    delta = self.tile_size - (\n        self.stream_network.tile_gradient_lost.left + self.stream_network.tile_gradient_lost.right\n    )\n    delta = delta // self.stream_network.output_stride[-1]\n    delta *= self.stream_network.output_stride[-1]\n    return delta.detach().cpu().numpy()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.disable_streaming","title":"<code>disable_streaming()</code>","text":"<p>Disable streaming hooks and replace streamingconv2d  with conv2d modules</p> <p>This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient checkpointing will be turned off.</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def disable_streaming(self):\n    \"\"\"Disable streaming hooks and replace streamingconv2d  with conv2d modules\n\n    This will still use the StreamingCNN backward and forward functions, but the memory gains from gradient\n    checkpointing will be turned off.\n    \"\"\"\n    self.stream_network.disable()\n    self.use_streaming = False\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.enable_streaming","title":"<code>enable_streaming()</code>","text":"<p>Enable streaming hooks and use streamingconv2d modules</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def enable_streaming(self):\n    \"\"\"Enable streaming hooks and use streamingconv2d modules\"\"\"\n    self.stream_network.enable()\n    self.use_streaming = True\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.forward_streaming","title":"<code>forward_streaming(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> required <code>The</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <code>The output of the streaming model</code> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def forward_streaming(self, x):\n    \"\"\"\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    The input tensor in [1,C,H,W] format\n\n    Returns\n    -------\n    out: torch.Tensor\n    The output of the streaming model\n\n    \"\"\"\n    out = self.stream_network(x) if self.use_streaming else self.stream_network.stream_module(x)\n    return out\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.freeze_streaming_normalization_layers","title":"<code>freeze_streaming_normalization_layers()</code>","text":"<p>Do not use normalization layers within lightstream, only local ops are allowed</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def freeze_streaming_normalization_layers(self):\n    \"\"\"Do not use normalization layers within lightstream, only local ops are allowed\"\"\"\n    freeze_layers = [l for l in self.stream_network.stream_module.modules() if isinstance(l, torch.nn.BatchNorm2d)]\n\n    for mod in freeze_layers:\n        mod.eval()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.get_trainable_params","title":"<code>get_trainable_params()</code>","text":"<p>Get trainable parameters for the entire model</p> <p>If self.streaming_layers is True, then the parameters of the streaming network will be trained. Otherwise, the parameters will be left untrained (no gradients will be collected)</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def get_trainable_params(self):\n    \"\"\" Get trainable parameters for the entire model\n\n    If self.streaming_layers is True, then the parameters of the streaming network will be trained.\n    Otherwise, the parameters will be left untrained (no gradients will be collected)\n\n    \"\"\"\n    if self.train_streaming_layers:\n        params = list(self._stream_module.parameters())\n        return params\n    else:\n        print(\"WARNING: Streaming network will not be trained\")\n        for param in self._stream_module.parameters():\n            param.requires_grad = False\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>on_train_epoch_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\" on_train_epoch_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    self.freeze_streaming_normalization_layers()\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_train_start","title":"<code>on_train_start()</code>","text":"<p>on_train_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_train_start(self):\n    \"\"\" on_train_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    # Update streaming to put all the inputs/tensors on the right device\n    self.stream_network.device = self.device\n    self.stream_network.mean = self.mean\n    self.stream_network.std = self.std\n    self.stream_network.dtype = self.dtype\n</code></pre>"},{"location":"modules/streamingmodule/#modules.streaming.StreamingModule.on_validation_start","title":"<code>on_validation_start()</code>","text":"<p>on_validation_start hook</p> <p>Do not override this method. Instead, call the parent class using super().on_train_start if you want to add this hook into your pipelines</p> Source code in <code>lightstream\\modules\\streaming.py</code> <pre><code>def on_validation_start(self):\n    \"\"\" on_validation_start hook\n\n    Do not override this method. Instead, call the parent class using super().on_train_start if you want\n    to add this hook into your pipelines\n\n    \"\"\"\n    # Update streaming to put all the inputs/tensors on the right device\n    self.stream_network.device = self.device\n    self.stream_network.mean = self.mean\n    self.stream_network.std = self.std\n    self.stream_network.dtype = self.dtype\n</code></pre>"},{"location":"tutorials/classification/","title":"Basic image classification","text":"<p>This tutorial briefly introduces the <code>model</code> repository to easily prototype streaming-capable models right off the bat. The workflow aims to follow the core design principles of the <code>lightning</code> framework, and will not deviate much from it.</p>"},{"location":"tutorials/classification/#training-a-resnet-architecture-using-streaming","title":"Training a ResNet architecture using streaming","text":"<p>For this example, we will use a ResNet-18 model architecture and train it on the Cifar-10 dataset. Although this dataset is small enough to train without streaming, we'll use it as a proof of concept. We assume that the reader is familiar with the regular workflow of a pytorch-lightning model. If this is not the case, please consult the lightning documentation for further information.</p>"},{"location":"tutorials/classification/#importing-the-relevant-packages","title":"Importing the relevant packages","text":"<pre><code>import os\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nimport lightning.pytorch as pl\nfrom models.resnet.resnet import StreamingResNet\n</code></pre> <p>We start by importing the relevant packages. The model repository inside the lightstream package comes with a streaming-capable version of the ResNet architectures.</p>"},{"location":"tutorials/classification/#defining-the-dataloader-and-model","title":"Defining the dataloader and model","text":"<p>The code below defines the datasets and dataloaders, as well as the model definition. The model is trained using the regular lightning workflow. The <code>StreamingResNet</code> class requires you to fill in the following arguments:   </p> <ul> <li><code>model_name</code>: A string defining the specific ResNet architecture, in this case ResNet-18</li> <li><code>tile_size</code>: 1600x1660: Streaming processes large images sequentially in tiles (or patches), which are stored in a later layer of the model, and then reconstructed into a whole feature map. Higher tile sizes will typically require more VRAM, but will speed up computations.</li> <li><code>loss_fn</code> : <code>torch.nn.functional.cross_entropy</code>. The loss function for the network. </li> <li><code>num_classes</code>: 10. The number of classes to predict. The default is 1000 classes (ImageNet) default. If a different number is specified, then the <code>fc</code> layer of the ResNet model is re-initialized with random weight and <code>num_classes</code> output neurons.</li> </ul> <pre><code># Define the dataset, and transform to tensor.\n# Since the images in the dataset are small, we don't need pyvips\ndataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\ntrain_loader = DataLoader(dataset, num_workers=3)\n\n#\nmodel = StreamingResNet(\n    \"resnet18\",  # model architecture arg\n    1600,        # the tile size\n    torch.nn.functional.cross_entropy, # the loss function, a pytorch object\n    num_classes=10\n)\n\n# train model\ntrainer = pl.Trainer(accelerator=\"gpu\")\ntrainer.fit(model=model, train_dataloaders=train_loader)\n</code></pre>"},{"location":"tutorials/custom_models/","title":"Custom models","text":"<p>Custom models can be created in one of three ways:</p> <ul> <li>Using the <code>StreamingModule</code> (recommended)</li> <li>Using the <code>BaseModel</code> (a subclass of <code>StreamingModule</code>)</li> <li>Creating your own class (not recommended)</li> </ul> <p>The <code>StreamingModule</code> and <code>BaseModel</code> classes are both regular <code>LightningModule</code> objects and should be treated as such. Both classes have several helper functions and a custom initialization that create the streaming model instance. Secondly,  the helper functions make sure that several settings, such as freezing normalization layers and setting them to <code>eval()</code> mode, both during training and inference. This is necessary since streaming does not work with layers that are not locally defined, but rather need the entire input image.</p> <p>Warning</p> <p>Please consult this documentation thoroughly before creating your own module. Do not carelessly override the following callbacks/hooks: on_train_epoch_start, on_train_start, on_validation_start, on_test_start</p>"},{"location":"tutorials/custom_models/#model-prerequisites","title":"Model prerequisites","text":"<p>Before implementing a streaming version of your model, make sure that the following conditions hold:</p> <ul> <li>Your model is at least partly a CNN</li> <li>Within the CNN, fully connected layers or global pooling layers are not used. This also means it should not have Squeeze and Excite (SE) blocks, since these are global operations, rather than local.<ul> <li>A small exception to this are normalization layers. If your model contains any, they must be set to <code>eval()</code> both during inference and training. Since most normalization require the entire input to correctly calculate means and standard deviations, they will theoretically not work with streaming during training. During inference, the means and standard deviations can be applied tile-wise.</li> </ul> </li> </ul>"},{"location":"tutorials/custom_models/#splitting-and-creating-the-models","title":"Splitting and creating the model(s)","text":"<p>Usually a CNN contains a part that can be streamed, and a part that cannot be used with streaming. In the case of e.g. a ResNet architecture, the <code>fc</code> layers of the model contains global pooling and fully connected layers, and thus are not fit for streaming.  That's why models are usually split. The correct model can then be made streamable using the <code>StreamingModule</code>\"</p> <pre><code>def split_resnet(net, **kwargs):\n    \"\"\" Split resnet architectures into backbone and fc modules\n\n    The stream_net will contain the CNN backbone that is capable for streaming.\n    The fc model is not streamable and will be a separate module\n    If num_classes are specified as a kwarg, then a new fc model will be created with the desired classes\n\n    Parameters\n    ----------\n    net: torch model\n        A ResNet model in the format provided by torchvision\n    kwargs\n\n    Returns\n    -------\n    stream_net : torch.nn.Sequential\n        The CNN backbone of the ResNet\n    head : torch.nn.Sequential\n        The head of the model, defaults to the fc model provided by torchvision.\n\n    \"\"\"\n\n\n    num_classes = kwargs.get(\"num_classes\", 1000)\n    stream_net = nn.Sequential(\n        net.conv1, net.bn1, net.relu, net.maxpool, net.layer1, net.layer2, net.layer3, net.layer4\n    )\n\n    # 1000 classes is the default from ImageNet classification\n    if num_classes != 1000:\n        net.fc = torch.nn.Linear(512, num_classes)\n        torch.nn.init.xavier_normal_(net.fc.weight)\n        net.fc.bias.data.fill_(0)  # type:ignore\n\n    head = nn.Sequential(net.avgpool, nn.Flatten(), net.fc)\n\n    return stream_net, head\n</code></pre> <p>After defining the model split, the <code>StreamingModule</code> or <code>BaseModule</code> can be used to create a model consisting of a streamable CNN backbone and optional head networks. Both the  <code>StreamingModule</code> and <code>BaseModule</code> inherit from <code>lightning.LightningModule</code>, meaning all the regular pytorch lightning functions and workflows become available here well. The streamable ResNet is defined below:</p> <pre><code>class StreamingResNet(StreamingModule):\n    model_choices = {\"resnet18\": resnet18, \"resnet34\": resnet34, \"resnet50\": resnet50}\n\n    def __init__(\n        self,\n        model_name: str,\n        tile_size: int,\n        loss_fn: torch.nn.functional,\n        train_streaming_layers: bool = True,\n        use_streaming: bool = True,\n        *args,\n        **kwargs\n    ):\n        assert model_name in list(StreamingResNet.model_choices.keys())\n        network = StreamingResNet.model_choices[model_name](weights=\"IMAGENET1K_V1\")\n        stream_net, head = split_resnet(network, num_classes=kwargs.get(\"num_classes\"))\n        super().__init__(\n            stream_net,\n            head,\n            tile_size,\n            train_streaming_layers=train_streaming_layers,\n            use_streaming=use_streaming,\n            *args,\n            **kwargs\n        )\n</code></pre>"},{"location":"tutorials/custom_models/#custom-forwardbackward-logic","title":"Custom forward/backward logic","text":"<p>Since we inherit directory from the lightning modules, the routine for forward and backpropagation remains mostly similar to that of pytorch lightning. However, there are a few tweaks and tricks that must be taken into account:</p> <ul> <li>Forward pass: Can be run as a usual, but we recommend making the input image accessible via self: i.e. <code>self.image = image</code></li> </ul> <pre><code>def forward_head(self, x):\n    return self.head(x)\n\ndef forward(self, x):\n    fmap = self.forward_streaming(x)\n    out = self.forward_head(fmap)\n    return out\n</code></pre> <ul> <li>Backward pass/training step: The StreamingCNN object that defines the streaming network requires the gradient and input image as input. To actually backpropagate the input, you can use the <code>backward_streaming</code> function which requires the input image and the gradient of the head of the model.</li> </ul> <pre><code>def training_step(self, batch: Any, batch_idx: int, *args: Any, **kwargs: Any) -&gt; tuple[Any, Any, Any]:\n    image, target = batch\n\n    # This is needed later in the backward function!\n    self.image = image\n\n    self.str_output = self.forward_streaming(image)\n\n    if self.use_streaming:\n        self.str_output.requires_grad = self.training\n\n    out = self.forward_head(self.str_output)\n    loss = self.loss_fn(out, target)\n\n    self.log_dict({\"entropy loss\": loss.detach()}, prog_bar=True)\n    return loss\n\ndef backward(self, loss):\n    loss.backward()\n    if self.train_streaming_layers and self.use_streaming:\n        self.backward_streaming(self.image, self.str_output.grad)\n    del self.str_output\n</code></pre> <ul> <li>Hooks: Several hooks in pytorch lightning are used to set the normalization layers to <code>eval()</code> and set the inputs/models to the right device (this is not how it should be done, but we are working on a solution for this).<ul> <li>on_training_start: Allocates the input and models to the correct device at training time.</li> <li>on_validation_start: Allocates the input and models to the correct device at validation time.</li> <li>on_test_start: Allocates the input and models to the correct device at test time.</li> <li>on_train_epoch_start(self): sets all the normalization layers to eval() during training</li> </ul> </li> </ul> <p>Warning: do not override these hooks with your own code. If you need these hooks for any reason, then call the parent method first using e.g.  <code>super().on_training_start</code></p>"},{"location":"tutorials/dataloading/","title":"Image processing using pyvips and albumentationsxl","text":"<p>For this tutorial, we will be using the pyvips backend to load and manipulate images. Pyvips was specifically built with  large images in mind. It builds data pipelines for each image, rather than directly loading it into memory. As a result, it can keep a low memory footprint during execution, whilst still being fast. </p> <p>Secondly, we will be using the albumentationsxl package. This is virtually the same package as albumentations, but using the pyvips backend. It features a wide range of image augmentations capable of transforming large images specifically.</p> <p>Within this tutorial we will be using the Imagenette dataset to serve as an example. Notice that these images are small enough to fit in memory. The example below only serves as a demonstration of how the data augmentation works with a pyvips backend, and should also work with large images.</p>"},{"location":"tutorials/dataloading/#an-example-using-imagenette","title":"An example using Imagenette","text":""},{"location":"tutorials/dataloading/#downloading-and-extracting-imagenette-data","title":"Downloading and extracting ImageNette data","text":"<p>We start by downloading and extracting the ImageNette dataset into the data folder, which is stored in the current working directory. The data is then extracted using the <code>extract_all_files</code> function</p> <pre><code>import os\nimport pyvips\nimport albumentationsxl as A\nfrom pathlib import Path\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import Dataset\nimport tarfile\n\n\ndef extract_all_files(tar_file_path, extract_to):\n    with tarfile.open(tar_file_path, \"r\") as tar:\n        tar.extractall(extract_to)\n\n\ndef download_and_extract():\n    # Download dataset and extract in a data/ directory\n    if not os.path.isfile(\"data/imagenette2-320.tgz\"):\n        print(\"Downloading dataset\")\n        download_url(\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\", os.getcwd() + \"/data\")\n        print(\"Extracting dataset\")\n        extract_all_files(os.getcwd() + \"/data/imagenette2-320.tgz\", os.getcwd() + \"/data\")\n</code></pre>"},{"location":"tutorials/dataloading/#define-the-dataloader","title":"Define the dataloader","text":"<p>Defining a pytorch dataset with pyvips is straightforward and does not require much tweaking from normal pipelines. In a nutshell, the following changes are made from a normal pipeline:</p> <ul> <li>Instead of opening image files using PIL, torch, cv2, etc... we are opening the file with a pyvips backend <code>pyvips.Image.new_from_file()</code></li> <li>The albumentations backend is replaced with the albumentationsxl package. </li> </ul> <pre><code>class ImagenetteDataset(Dataset):\n    def __init__(self, patch_size=320, validation=False):\n        self.folder = Path(\"data/imagenette2-320/train\") if not validation else Path(\"data/imagenette2-320/val\")\n        self.classes = [ \"n01440764\", \"n02102040\", \"n02979186\", \"n03000684\", \"n03028079\", \"n03394916\", \"n03417042\",\n                         \"n03425413\", \"n03445777\", \"n03888257\"]\n\n        self.images = []\n        for cls in self.classes:\n            cls_images = list(self.folder.glob(cls + \"/*.JPEG\"))\n            self.images.extend(cls_images)\n\n        self.patch_size = patch_size\n        self.validation = validation\n\n        self.transforms = A.Compose(\n            [\n                A.RandomBrightnessContrast(p=1.0),\n                A.Rotate(p=0.8),\n                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                A.ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, index):\n        image_fname = self.images[index]\n        image = pyvips.Image.new_from_file(image_fname)\n        label = image_fname.parent.stem\n        label = self.classes.index(label)\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]\n\n        return image, label\n\n    def __len__(self):\n        return len(self.images)\n</code></pre> <p>Creating custom datasets with large images thus requires little changes and remain flexible enough to use with most of the Pytorch native infrastructure outside of image transformations. Since these are regular Pytorch Dataset objects, it will work with any data, including masks, bounding boxes, or keypoints, as long as they can be converted into Tensors for model training.</p>"},{"location":"tutorials/dataloading/#image-processing-best-practices","title":"Image processing best practices","text":"<p>The dataset used in the example enough was small enough to load into memory using normal image libraries. This was done to keep the tutorial lightweight and easy to execute. In practice larger images will be more of a challenge to optimize correctly. Below we provide several tips to include in your custom pipeline to facilitate fast image processing with lower memory footprints.</p>"},{"location":"tutorials/dataloading/#load-images-as-uint8","title":"load images as uint8","text":"<p>Pyvips images can be loaded or otherwise cast to uint8 (\"uchar\" in pyvips). This will increase the speed of the computations done by pyvips, while also preserving memory. </p>"},{"location":"tutorials/dataloading/#number-of-transformations-and-sequence","title":"Number of transformations and sequence","text":"<p>Image transformations on large images are costly, therefore, make sure not to include any redundant, e.g. mixing <code>Affine</code> with <code>Rotate</code> in the same pipeline, as Rotate is a subset of Affine. Also, some transformations are computationally expensive, such as elastic transforms, so try to avoid using this transformation every time if you are experiencing poor GPU utilization due to cpu bottlenecks.</p> <p>Finally, the <code>Normalize</code> transform will cast the image to a <code>float32</code> format. It is recommended to always put this transformation into the very end of the augmentation pipeline, since float32 operations are costlier than <code>uint8</code>. Failing to do so can introduce bottlenecks in the augmentation pipeline.</p>"},{"location":"tutorials/dataloading/#optional-image-normalization-on-gpu","title":"Optional: image normalization on gpu","text":"<p>within the streaming library, it is possible to normalize the image tile-wise on the gpu instead of in the dataloader. This could improve speed, as well as  lower memory footprints in some scenarios, for example if the image can be stored as an uint8 tensor before converting to e.g. float16. </p>"},{"location":"tutorials/dataloading/#image-memory-requirements","title":"Image memory requirements","text":"<p>We can take a look at the following table on how images grow in size as we increase their size, as well as changing their dtypes. From this table, we can already conclude that it is better to work with uint8 and float16 for training as much a possible. </p> Table 1: All values are in gigabyes (GB). Values generated using random numpy arrays Image size uint8 float16 float32 float64 8192x8192x3 0.2 0.4 0.8 1.6 16384x16384x3 0.8 1.6 3.2 6.4 32768x32768x3 3.2 6.4 12.8 25.6 65536x65536x3 12.8 25.6 51.2 102.4"},{"location":"tutorials/trainer_options/","title":"Trainer settings","text":"<p>The <code>Trainer</code> settings defined in native Pytorch Lightning will</p>"},{"location":"tutorials/trainer_options/#multi-gpu-support","title":"Multi GPU support","text":"<p>Training on multiple gpu's in streaming is handled by Pytorch lightning. However, the native <code>auto</code> option within the <code>Trainer</code> class will not work (we omit the technical details why here). For the <code>strategy</code> argument of the Trainer, we therefore recommend to use the <code>ddp_find_unused_parameters_true</code> instead, which does not conflict with streaming and gradient checkpointing.</p>"},{"location":"tutorials/trainer_options/#gradient-accumulation","title":"Gradient accumulation","text":"<p>Specifying higher batch sizes will not affect normalization layers during training, as they should be on <code>eval()</code> mode. However, gradient accumulation is still possible and can stabilize training under certain circumstances. This can be easily set using the <code>accumulate_grad_batches</code> argument.</p>"},{"location":"tutorials/trainer_options/#precision","title":"Precision","text":"<p>We recommend to train using mixed precision training wherever possible and to let pytorch handle the conversion. This can be set using the <code>16-mixed</code> option.</p>"},{"location":"tutorials/trainer_options/#loggers-and-callbacks","title":"Loggers and callbacks","text":"<p>Callbacks for a variety of training strategies (checkpointing, early stopping, etc) are natively supported by Pytorch Lightning. Please consult their respective documentation on how to do this. The same can be said for standard logging solutions (Tensorboard, Wandb).</p>"},{"location":"tutorials/trainer_options/#example","title":"Example","text":"<pre><code>trainer = pl.Trainer(\n    default_root_dir=\"path_to_save_dir\",\n    accelerator=\"gpu\",\n    max_epochs=100,\n    devices=2,\n    strategy=\"ddp_find_unused_parameters_true\",\n    accumulate_grad_batches=8,\n    precision=\"16-mixed\",\n    logger=wandb_logger,\n)\n</code></pre>"}]}